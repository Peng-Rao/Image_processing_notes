{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "z4OrCm2aFXhE"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from scipy.io import loadmat\n",
    "from skimage.io import imread\n",
    "\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format='retina'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VKVyjdHyIvBa"
   },
   "outputs": [],
   "source": [
    "rootfolder = \"..\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-NB1-MvmFsot"
   },
   "source": [
    "Useful function for plot a 2D dictionary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Rey7kIlUF22r"
   },
   "outputs": [],
   "source": [
    "def get_dictionary_img(D):\n",
    "    M, N = D.shape\n",
    "    p = int(round(np.sqrt(M)))\n",
    "    nnn = int(np.ceil(np.sqrt(N)))\n",
    "    bound = 2\n",
    "    img = np.ones((nnn * p + bound * (nnn - 1), nnn * p + bound * (nnn - 1)))\n",
    "    for i in range(N):\n",
    "        m = np.mod(i, nnn)\n",
    "        n = int((i - m) / nnn)\n",
    "        m = m * p + bound * m\n",
    "        n = n * p + bound * n\n",
    "        atom = D[:, i].reshape((p, p))\n",
    "        if atom.min() < atom.max():\n",
    "            atom = (atom - atom.min()) / (atom.max() - atom.min())\n",
    "        img[m : m + p, n : n + p] = atom\n",
    "\n",
    "    return img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "elxPQr5jRssy"
   },
   "source": [
    "Define a function that implements the OMP\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HEEtx02YRxui"
   },
   "outputs": [],
   "source": [
    "def OMP(s, D, L, tau):\n",
    "    _, N = D.shape\n",
    "    r = s.copy()  # initial residual\n",
    "    omega = []  # support set\n",
    "    x_OMP = np.zeros(N)  # final sparse code\n",
    "\n",
    "    while len(omega) < L and np.linalg.norm(r) > tau:\n",
    "        # SWEEP STEP: compute correlations between residual and dictionary atoms\n",
    "        e = np.zeros(N)\n",
    "        for j in range(N):\n",
    "            e[j] = D[:, j].T @ r\n",
    "\n",
    "        # find the column index with maximum correlation\n",
    "        jStar = np.argmax(np.abs(e))\n",
    "\n",
    "        # UPDATE support set\n",
    "        if jStar not in omega:\n",
    "            omega.append(jStar)\n",
    "\n",
    "        # update coefficients using least squares\n",
    "        D_omega = D[:, omega]\n",
    "        x_omega, _, _, _ = np.linalg.lstsq(D_omega, s, rcond=None)\n",
    "\n",
    "        # update residual\n",
    "        r = s - D_omega @ x_omega\n",
    "\n",
    "    # construct full sparse vector\n",
    "    for i, idx in enumerate(omega):\n",
    "        x_OMP[idx] = x_omega[i]\n",
    "\n",
    "    return x_OMP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ENFQwMcaI9aR"
   },
   "source": [
    "## Dictionary Learning\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XF0xLVZyGCBc"
   },
   "source": [
    "Load the image and rescale it in $[0,1]$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JZXPMFW2F-W1"
   },
   "outputs": [],
   "source": [
    "path_image = (\n",
    "    f\"{rootfolder}/data/barbara.png\"  #  barbara.png, cameraman.png, Lena512.png\n",
    ")\n",
    "\n",
    "img = imread(path_image) / 255\n",
    "\n",
    "imsz = img.shape\n",
    "\n",
    "# patch size\n",
    "p = 8\n",
    "\n",
    "# number of elements in the patch\n",
    "M = p**2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KRBSmSG5GIPe"
   },
   "source": [
    "Extract a bunch of random patches from the image and build the training set $S$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "m1NfJRxKGKFu"
   },
   "outputs": [],
   "source": [
    "npatch = 10000\n",
    "\n",
    "S = np.zeros((M, npatch))\n",
    "for i in range(npatch):\n",
    "    # Random top-left corner for patch\n",
    "    row = np.random.randint(0, imsz[0] - p + 1)\n",
    "    col = np.random.randint(0, imsz[1] - p + 1)\n",
    "    patch = img[row : row + p, col : col + p]\n",
    "    S[:, i] = patch.flatten()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y4O1zq_JGPeL"
   },
   "source": [
    "Remove the mean from the patches (each column of $S$ must have zero-mean)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9OtzbfA_GRIA"
   },
   "outputs": [],
   "source": [
    "S = S - np.mean(S, axis=0, keepdims=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q6MOxJJOTuK1"
   },
   "source": [
    "Define a function that implements the KSVD\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ue9PsXb9Txk2"
   },
   "outputs": [],
   "source": [
    "def ksvd(S, M, N, max_iter, npatch, L, print_time=False):\n",
    "    # initialize the dictionary\n",
    "    D = np.random.randn(M, N)\n",
    "\n",
    "    # normalize each column of D (zero mean and unit norm)\n",
    "    # UPDATE D\n",
    "    D = D - np.mean(D, axis=0, keepdims=True)\n",
    "    D = D / np.linalg.norm(D, axis=0, keepdims=True)\n",
    "\n",
    "    # initialize the coefficient matrix\n",
    "    X = np.zeros((N, npatch))\n",
    "\n",
    "    # Main KSVD loop\n",
    "    for iter in range(max_iter):\n",
    "        time_start = time.time()\n",
    "\n",
    "        # Sparse coding step\n",
    "        # perform the sparse coding via OMP of all the columns of S\n",
    "        for n in range(npatch):\n",
    "            X[:, n] = OMP(S[:, n], D, L, 1e-6)\n",
    "\n",
    "        # Dictionary update step\n",
    "        # iterate over the columns of D\n",
    "        for j in range(N):\n",
    "            # find which signals uses the j-th atom in the sparse coding\n",
    "            omega = np.where(X[j, :] != 0)[0]\n",
    "\n",
    "            if len(omega) == 0:\n",
    "                # if the atom is never used then ignore or substitute it with a random vector\n",
    "                D[:, j] = np.random.randn(M)\n",
    "                D[:, j] = D[:, j] / np.linalg.norm(D[:, j])\n",
    "            else:\n",
    "                # compute the residual matrix E, ignoring the j-th atom\n",
    "                E = S - D @ X + np.outer(D[:, j], X[j, :])\n",
    "\n",
    "                # restrict E to the columns indicated by omega\n",
    "                Eomega = E[:, omega]\n",
    "\n",
    "                # Compute the best rank-1 approximation\n",
    "                U, Sigma, Vt = np.linalg.svd(Eomega, full_matrices=False)\n",
    "\n",
    "                # update the dictionary\n",
    "                D[:, j] = U[:, 0]\n",
    "\n",
    "                # update the coefficient matrix\n",
    "                X[j, omega] = Sigma[0] * Vt[0, :]\n",
    "\n",
    "        time_end = time.time()\n",
    "        if print_time:\n",
    "            print(f\"Iteration {iter} runtime: {time_end - time_start}\")\n",
    "\n",
    "    return D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BgGdWipTTXW-"
   },
   "outputs": [],
   "source": [
    "# number of columns of the dictionary\n",
    "N = 256\n",
    "\n",
    "# number of iteration of the KSVD\n",
    "max_iter = 10\n",
    "\n",
    "# maximum number of nonzero coefficients for the sparse coding\n",
    "L = 4\n",
    "\n",
    "\n",
    "# Call the KSVD implementation\n",
    "D = ksvd(S, M, N, max_iter, npatch, L, print_time=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UhGFrleuTzyn"
   },
   "source": [
    "Show the learned dictionary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iZV1qd-DT5vG"
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 2, figsize=(20, 10))\n",
    "\n",
    "ax[0].imshow(img, cmap=\"gray\")\n",
    "ax[0].set_title(f\"Image {path_image.split('/')[-1]}\")\n",
    "ax[0].axis(\"off\")\n",
    "\n",
    "img_dict = get_dictionary_img(D)\n",
    "ax[1].imshow(img_dict, cmap=\"gray\")\n",
    "ax[1].set_title(f\"Dictionary learned from {path_image.split('/')[-1]}\")\n",
    "ax[1].axis(\"off\")\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i8fH4GcMPSpN"
   },
   "source": [
    "## OMP denoising with learned dictionaries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "h7e70IsjPc81"
   },
   "outputs": [],
   "source": [
    "img_clean = imread(f\"{rootfolder}/data/barbara.png\") / 255\n",
    "\n",
    "# Corrupt the image\n",
    "\n",
    "sigma_noise = 20 / 255\n",
    "noisy_img = img_clean + np.random.normal(size=imsz) * sigma_noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sAFV-D1qPsyW"
   },
   "outputs": [],
   "source": [
    "psnr_noisy = 10 * np.log10(1 / np.mean((noisy_img - img_clean) ** 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xNB_n9joPyFE"
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 2, figsize=(20, 10))\n",
    "ax[0].imshow(img_clean, cmap=\"gray\")\n",
    "ax[0].set_title(\"Original image (barbara.png)\")\n",
    "ax[0].axis(\"off\")\n",
    "\n",
    "ax[1].imshow(noisy_img, cmap=\"gray\")\n",
    "ax[1].set_title(f\"Noisy image, PSNR = {psnr_noisy:.2f}\")\n",
    "ax[1].axis(\"off\")\n",
    "\n",
    "plt.tight_layout()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0-9ZG5o5P1Q3"
   },
   "outputs": [],
   "source": [
    "# patch size\n",
    "p = 8\n",
    "\n",
    "# number of elements in the patch\n",
    "M = p**2\n",
    "\n",
    "# number of columns of the dictionary\n",
    "N = 256\n",
    "\n",
    "# number of iteration of the KSVD\n",
    "max_iter = 10\n",
    "\n",
    "# maximum number of nonzero coefficients for the sparse coding\n",
    "L = 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s5K58xgmQCQM"
   },
   "source": [
    "Generic dictionary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6bcyFFOTP_0n"
   },
   "outputs": [],
   "source": [
    "D_generic = loadmat(f\"{rootfolder}/data/dict_nat_img.mat\")[\"D\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dmwEZ4-8QEvD"
   },
   "source": [
    "Dictionary learned from a different image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tNjWo3UrQFJq"
   },
   "outputs": [],
   "source": [
    "img = imread(f\"{rootfolder}/data/cameraman.png\") / 255\n",
    "\n",
    "# Extract random patches\n",
    "npatch = 10000\n",
    "\n",
    "S = np.zeros((M, npatch))\n",
    "for i in range(npatch):\n",
    "    # Random top-left corner for patch\n",
    "    row = np.random.randint(0, img.shape[0] - p + 1)\n",
    "    col = np.random.randint(0, img.shape[1] - p + 1)\n",
    "    patch = img[row : row + p, col : col + p]\n",
    "    S[:, i] = patch.flatten()\n",
    "\n",
    "S = S - np.mean(S, axis=0, keepdims=True)\n",
    "\n",
    "# Learn the dictionary\n",
    "D_diff = ksvd(S, M, N, max_iter, npatch, L, print_time=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-nCtt-K1QYsf"
   },
   "source": [
    "Dictionary learned from the noisy image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-4wkSoNWQY7y"
   },
   "outputs": [],
   "source": [
    "# Extract random patches\n",
    "npatch = 10000\n",
    "\n",
    "S = np.zeros((M, npatch))\n",
    "for i in range(npatch):\n",
    "    # Random top-left corner for patch\n",
    "    row = np.random.randint(0, noisy_img.shape[0] - p + 1)\n",
    "    col = np.random.randint(0, noisy_img.shape[1] - p + 1)\n",
    "    patch = noisy_img[row : row + p, col : col + p]\n",
    "    S[:, i] = patch.flatten()\n",
    "\n",
    "S = S - np.mean(S, axis=0, keepdims=True)\n",
    "\n",
    "# Learn the dictionary\n",
    "D_noisy = ksvd(S, M, N, max_iter, npatch, L, print_time=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IoUlEjIiQqFq"
   },
   "source": [
    "Dictionary learned from the clean image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lAALKoBGQqlF"
   },
   "outputs": [],
   "source": [
    "# Extract random patches\n",
    "npatch = 10000\n",
    "\n",
    "S = np.zeros((M, npatch))\n",
    "for i in range(npatch):\n",
    "    # Random top-left corner for patch\n",
    "    row = np.random.randint(0, img_clean.shape[0] - p + 1)\n",
    "    col = np.random.randint(0, img_clean.shape[1] - p + 1)\n",
    "    patch = img_clean[row : row + p, col : col + p]\n",
    "    S[:, i] = patch.flatten()\n",
    "\n",
    "S = S - np.mean(S, axis=0, keepdims=True)\n",
    "\n",
    "# Learn the dictionary\n",
    "D_clean = ksvd(S, M, N, max_iter, npatch, L, print_time=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uE4yTyzsQzb-"
   },
   "source": [
    "OMP denoising\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GjcuX3ifQz8k"
   },
   "outputs": [],
   "source": [
    "def omp_denoising(noisy_img, D, step, tau):\n",
    "    # Get image dimensions and patch size\n",
    "    imsz = noisy_img.shape\n",
    "    M, N = D.shape\n",
    "    p = int(np.sqrt(M))  # patch size (assuming square patches)\n",
    "\n",
    "    # Initialize the estimated image and weight matrix\n",
    "    img_hat = np.zeros(imsz)\n",
    "    weights = np.zeros(imsz)\n",
    "\n",
    "    # Operate patchwise\n",
    "    for i in range(0, imsz[0] - p + 1, step):\n",
    "        for j in range(0, imsz[1] - p + 1, step):\n",
    "            # Extract the patch with the top left corner at pixel (i, j)\n",
    "            s = noisy_img[i : i + p, j : j + p].flatten()\n",
    "\n",
    "            # Store and subtract the mean\n",
    "            s_mean = s.mean()\n",
    "            s -= s_mean\n",
    "\n",
    "            # Perform the sparse coding\n",
    "            x = OMP(s, D, L=10, tau=tau)\n",
    "\n",
    "            # Perform the reconstruction\n",
    "            s_hat = D @ x\n",
    "\n",
    "            # Add back the mean\n",
    "            s_hat += s_mean\n",
    "\n",
    "            # Put the denoised patch into the estimated image\n",
    "            img_hat[i : i + p, j : j + p] += s_hat.reshape(p, p)\n",
    "\n",
    "            # Store the weight of the current patch in the weight matrix\n",
    "            weights[i : i + p, j : j + p] += 1\n",
    "\n",
    "    # Normalize the estimated image with the computed weights\n",
    "    img_hat = img_hat / weights\n",
    "\n",
    "    return img_hat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WeTv2cH2Q8My"
   },
   "source": [
    "Denoising using the learned dictionaries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "izTArKuqQ8zG"
   },
   "outputs": [],
   "source": [
    "# set the threshold\n",
    "tau = 1.15 * p * sigma_noise\n",
    "\n",
    "# define the step (=p for non overlapping paches)\n",
    "STEP = 4  # STEP = 1 might be very time consuming, start with larger STEP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SPbxHnZJRFEi"
   },
   "source": [
    "Solve the four denoising problems\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "h7zpEcMTRHNa"
   },
   "outputs": [],
   "source": [
    "# Denoising with dictionary D_generic\n",
    "img_hat_generic = omp_denoising(noisy_img, D_generic, STEP, tau)\n",
    "\n",
    "# Denoising with dictionary D_diff\n",
    "img_hat_diff = omp_denoising(noisy_img, D_diff, STEP, tau)\n",
    "\n",
    "# Denoising with dictionary D_noisy\n",
    "img_hat_noisy = omp_denoising(noisy_img, D_noisy, STEP, tau)\n",
    "\n",
    "# Denoising with dictionary D_clean\n",
    "img_hat_clean = omp_denoising(noisy_img, D_clean, STEP, tau)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PGdkd7VgRgzo"
   },
   "source": [
    "Visualize the results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "R42tSb7ORhTk"
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 2, figsize=(20, 10))\n",
    "ax[0].imshow(get_dictionary_img(D_generic), cmap=\"gray\")\n",
    "ax[0].set_title(\"Dictionary (Generic)\")\n",
    "ax[0].axis(\"off\")\n",
    "\n",
    "psnr_hat = 10 * np.log10(1 / np.mean((img_hat_generic - img_clean) ** 2))\n",
    "\n",
    "ax[1].imshow(img_hat_generic, cmap=\"gray\")\n",
    "ax[1].set_title(f\"Denoised image, PSNR = {psnr_hat:.2f}\")\n",
    "ax[1].axis(\"off\")\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HE3On_JdRpIa"
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 2, figsize=(20, 10))\n",
    "ax[0].imshow(get_dictionary_img(D_diff), cmap=\"gray\")\n",
    "ax[0].set_title(\"Dictionary (From a different image)\")\n",
    "ax[0].axis(\"off\")\n",
    "\n",
    "psnr_hat = 10 * np.log10(1 / np.mean((img_hat_diff - img_clean) ** 2))\n",
    "\n",
    "ax[1].imshow(img_hat_diff, cmap=\"gray\")\n",
    "ax[1].set_title(f\"Denoised image, PSNR = {psnr_hat:.2f}\")\n",
    "ax[1].axis(\"off\")\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ggAh1ReERr-R"
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 2, figsize=(20, 10))\n",
    "ax[0].imshow(get_dictionary_img(D_noisy), cmap=\"gray\")\n",
    "ax[0].set_title(\"Dictionary (From the noisy image)\")\n",
    "ax[0].axis(\"off\")\n",
    "\n",
    "psnr_hat = 10 * np.log10(1 / np.mean((img_hat_noisy - img_clean) ** 2))\n",
    "\n",
    "ax[1].imshow(img_hat_noisy, cmap=\"gray\")\n",
    "ax[1].set_title(f\"Denoised image, PSNR = {psnr_hat:.2f}\")\n",
    "ax[1].axis(\"off\")\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1Y-bM9jORwCc"
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 2, figsize=(20, 10))\n",
    "ax[0].imshow(get_dictionary_img(D_clean), cmap=\"gray\")\n",
    "ax[0].set_title(\"Dictionary (from the clean image)\")\n",
    "ax[0].axis(\"off\")\n",
    "\n",
    "psnr_hat = 10 * np.log10(1 / np.mean((img_hat_clean - img_clean) ** 2))\n",
    "\n",
    "ax[1].imshow(img_hat_clean, cmap=\"gray\")\n",
    "ax[1].set_title(f\"Denoised image, PSNR = {psnr_hat:.2f}\")\n",
    "ax[1].axis(\"off\")\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Download few texture-rich images from the Brodatz dataset\n",
    "- Use KSVD to learn dictionaries from these images\n",
    "- Try with different patch sizes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Brodatz texture image\n",
    "brodatz_img = imread(f\"{rootfolder}/data/1.1.03.tiff\") / 255\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.imshow(brodatz_img, cmap=\"gray\")\n",
    "plt.title(\"Brodatz Texture Image\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary learning parameters\n",
    "N = 256  # dictionary size\n",
    "max_iter = 10\n",
    "L = 4  # sparsity level\n",
    "npatch = 10000\n",
    "\n",
    "# Try different patch sizes\n",
    "patch_sizes = [4, 8, 16]\n",
    "dictionaries = {}\n",
    "\n",
    "for p in patch_sizes:\n",
    "    print(f\"\\nLearning dictionary with patch size {p}x{p}\")\n",
    "\n",
    "    M = p**2  # number of elements in patch\n",
    "\n",
    "    # Extract random patches\n",
    "    S = np.zeros((M, npatch))\n",
    "    for i in range(npatch):\n",
    "        row = np.random.randint(0, brodatz_img.shape[0] - p + 1)\n",
    "        col = np.random.randint(0, brodatz_img.shape[1] - p + 1)\n",
    "        patch = brodatz_img[row : row + p, col : col + p]\n",
    "        S[:, i] = patch.flatten()\n",
    "\n",
    "    # Remove mean from patches\n",
    "    S = S - np.mean(S, axis=0, keepdims=True)\n",
    "\n",
    "    # Learn dictionary using K-SVD\n",
    "    D_brodatz = ksvd(S, M, N, max_iter, npatch, L, print_time=True)\n",
    "    dictionaries[p] = D_brodatz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize learned dictionaries\n",
    "fig, axes = plt.subplots(1, 3, figsize=(20, 6))\n",
    "\n",
    "for i, p in enumerate(patch_sizes):\n",
    "    dict_img = get_dictionary_img(dictionaries[p])\n",
    "    axes[i].imshow(dict_img, cmap=\"gray\")\n",
    "    axes[i].set_title(f\"Dictionary learned from Brodatz texture\\nPatch size: {p}x{p}\")\n",
    "    axes[i].axis(\"off\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Use KSVD to learn the dictionary ùê∑ from the clean image\n",
    "- Use this image-specific dictionary to perform inpainting\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load clean image for inpainting\n",
    "img_inpaint = imread(f\"{rootfolder}/data/barbara.png\") / 255\n",
    "\n",
    "# Create a mask for inpainting (simulate missing pixels)\n",
    "mask = np.ones_like(img_inpaint)\n",
    "np.random.seed(42)  # for reproducibility\n",
    "missing_ratio = 0.3  # 30% missing pixels\n",
    "missing_indices = np.random.choice(\n",
    "    img_inpaint.size, int(missing_ratio * img_inpaint.size), replace=False\n",
    ")\n",
    "mask_flat = mask.flatten()\n",
    "mask_flat[missing_indices] = 0\n",
    "mask = mask_flat.reshape(img_inpaint.shape)\n",
    "# Create damaged image\n",
    "damaged_img = img_inpaint * mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "\n",
    "axes[0].imshow(img_inpaint, cmap=\"gray\")\n",
    "axes[0].set_title(\"Original Image\")\n",
    "axes[0].axis(\"off\")\n",
    "\n",
    "axes[1].imshow(damaged_img, cmap=\"gray\")\n",
    "axes[1].set_title(\"Damaged Image (30% missing pixels)\")\n",
    "axes[1].axis(\"off\")\n",
    "\n",
    "axes[2].imshow(mask, cmap=\"gray\")\n",
    "axes[2].set_title(\"Mask (white = known, black = missing)\")\n",
    "axes[2].axis(\"off\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use KSVD to learn the dictionary ùê∑ from the clean image\n",
    "D_inpaint = D_clean.copy()\n",
    "# Prepare dictionary for inpainting (add DC component)\n",
    "M, N = D_inpaint.shape\n",
    "dc = np.ones((M, 1)) / np.sqrt(M)\n",
    "D_inpaint = np.hstack([D_inpaint, dc])\n",
    "D_inpaint = D_inpaint / np.linalg.norm(D_inpaint, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def omp_inpainting(damaged_img, mask, D, step, sigma_noise=0.02):\n",
    "    imsz = damaged_img.shape\n",
    "    M, N = D.shape\n",
    "    p = int(np.sqrt(M))  # patch size\n",
    "\n",
    "    # SET stopping criteria of OMP\n",
    "    L = M // 2  # maximum sparsity level\n",
    "\n",
    "    # Initialize the estimated image and weight matrix\n",
    "    img_hat = np.zeros_like(damaged_img)\n",
    "    weights = np.zeros_like(damaged_img)\n",
    "\n",
    "    # Operate patchwise\n",
    "    for i in range(0, imsz[0] - p + 1, step):\n",
    "        for j in range(0, imsz[1] - p + 1, step):\n",
    "            # Extract the patch with the top left corner at pixel (i, j)\n",
    "            s = damaged_img[i : i + p, j : j + p].ravel()\n",
    "\n",
    "            # Patch extracted from the mask\n",
    "            m = mask[i : i + p, j : j + p].ravel()\n",
    "\n",
    "            # Design the projection operator over the current patch\n",
    "            proj = np.diag(m)\n",
    "\n",
    "            # Tau should be proportional to the number of pixels remaining in the patch\n",
    "            missing_pixels = np.sum(m == 0)\n",
    "            if missing_pixels < p**2:  # Only process if some pixels are missing\n",
    "                delta_i = (\n",
    "                    1.15 * p * sigma_noise * np.sqrt((p**2 - missing_pixels) / p**2)\n",
    "                )\n",
    "\n",
    "                # Sparse coding w.r.t. PD (projected dictionary)\n",
    "                PD = proj @ D\n",
    "                x = OMP(proj @ s, PD, L, delta_i)\n",
    "\n",
    "                # Reconstruction: synthesis w.r.t. D yielding sparse representation\n",
    "                s_hat = D @ x\n",
    "\n",
    "                # Use uniform weights for aggregation\n",
    "                w = 1\n",
    "\n",
    "                # Put the reconstructed patch into the estimated image\n",
    "                img_hat[i : i + p, j : j + p] += w * s_hat.reshape(p, p)\n",
    "\n",
    "                # Store the weight of the current patch in the weight matrix\n",
    "                weights[i : i + p, j : j + p] += w\n",
    "\n",
    "    # Normalize the estimated image with the computed weights\n",
    "    # Avoid division by zero\n",
    "    weights[weights == 0] = 1\n",
    "    img_hat = img_hat / weights\n",
    "\n",
    "    return img_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform inpainting\n",
    "STEP_INPAINT = 2  # Step size for patch processing\n",
    "img_inpainted = omp_inpainting(damaged_img, mask, D_inpaint, STEP_INPAINT)\n",
    "\n",
    "# Calculate PSNR\n",
    "psnr_damaged = 10 * np.log10(1 / np.mean((damaged_img - img_inpaint) ** 2))\n",
    "psnr_inpainted = 10 * np.log10(1 / np.mean((img_inpainted - img_inpaint) ** 2))\n",
    "\n",
    "# Visualize results\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "\n",
    "axes[0].imshow(img_inpaint, cmap=\"gray\")\n",
    "axes[0].set_title(\"Original Image\")\n",
    "axes[0].axis(\"off\")\n",
    "\n",
    "axes[1].imshow(damaged_img, cmap=\"gray\")\n",
    "axes[1].set_title(f\"Damaged Image\\nPSNR = {psnr_damaged:.2f} dB\")\n",
    "axes[1].axis(\"off\")\n",
    "\n",
    "axes[2].imshow(img_inpainted, cmap=\"gray\")\n",
    "axes[2].set_title(f\"Inpainted Image\\nPSNR = {psnr_inpainted:.2f} dB\")\n",
    "axes[2].axis(\"off\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(np.abs(img_inpainted - img_clean), cmap=\"hot\")\n",
    "plt.colorbar()\n",
    "plt.title(\"Absolute Error After Inpainting\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Image_processing_notes",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
