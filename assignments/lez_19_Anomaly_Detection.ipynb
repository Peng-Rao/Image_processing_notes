{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3Q_m4cCiDehI"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from scipy.io import loadmat\n",
    "from skimage.io import imread"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dCOIY19mDxRR"
   },
   "outputs": [],
   "source": [
    "rootfolder = \"..\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-NB1-MvmFsot"
   },
   "source": [
    "Useful function for plot the dictionary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Rey7kIlUF22r"
   },
   "outputs": [],
   "source": [
    "def get_dictionary_img(D):\n",
    "    M, N = D.shape\n",
    "    p = int(round(np.sqrt(M)))\n",
    "    nnn = int(np.ceil(np.sqrt(N)))\n",
    "    bound = 2\n",
    "    img = np.ones((nnn * p + bound * (nnn - 1), nnn * p + bound * (nnn - 1)))\n",
    "    for i in range(N):\n",
    "        m = np.mod(i, nnn)\n",
    "        n = int((i - m) / nnn)\n",
    "        m = m * p + bound * m\n",
    "        n = n * p + bound * n\n",
    "        atom = D[:, i].reshape((p, p))\n",
    "        if atom.min() < atom.max():\n",
    "            atom = (atom - atom.min()) / (atom.max() - atom.min())\n",
    "        img[m : m + p, n : n + p] = atom\n",
    "\n",
    "    return img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QBmBkQRSEBFw"
   },
   "source": [
    "Set all the parameters for the anomaly detection\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SOQHJpvHD8v3"
   },
   "outputs": [],
   "source": [
    "# patch size (tha patch is square)\n",
    "p = 15\n",
    "\n",
    "# number of patches in the training set for dictionary learning\n",
    "npatch_dictionary = 10000\n",
    "\n",
    "# number of patches to estimate the confidence region\n",
    "npatch_region = 1000\n",
    "\n",
    "# parameters for the dictionary learning using the KSVD\n",
    "niter_dl = 10\n",
    "natom = int(np.round(p**2 * 1.5))\n",
    "L = 4\n",
    "\n",
    "# regularization parameters for the l1 sparse coding\n",
    "lmbda = 0.18"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "acWegV3eIXmN"
   },
   "source": [
    "## Construct the training and validation sets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "g32ca1edFYgA"
   },
   "outputs": [],
   "source": [
    "# load the training image and rescale it in [0,1]\n",
    "img = imread(f\"{rootfolder}/data/img_normal.png\") / 255\n",
    "\n",
    "# extract random patches from the image and store them in matrices S, V\n",
    "imsz = img.shape\n",
    "M = p**2\n",
    "\n",
    "# Training patches S\n",
    "S = np.zeros((M, npatch_dictionary))\n",
    "for n in range(npatch_dictionary):\n",
    "    i = np.random.randint(0, imsz[0] - p + 1)\n",
    "    j = np.random.randint(0, imsz[1] - p + 1)\n",
    "    patch = img[i : i + p, j : j + p]\n",
    "    S[:, n] = patch.flatten()\n",
    "\n",
    "# Validation patches V\n",
    "V = np.zeros((M, npatch_region))\n",
    "for n in range(npatch_region):\n",
    "    i = np.random.randint(0, imsz[0] - p + 1)\n",
    "    j = np.random.randint(0, imsz[1] - p + 1)\n",
    "    patch = img[i : i + p, j : j + p]\n",
    "    V[:, n] = patch.flatten()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y0JL30HzE76w"
   },
   "source": [
    "## Dictionary Learning\n",
    "\n",
    "Perform preprocessing on the patches in $S$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ppQOaVIlTjGW"
   },
   "outputs": [],
   "source": [
    "# PREPROCESSING: exclude black patches from S\n",
    "v = np.median(S, axis=0)\n",
    "S = S[:, v > 0.06]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qemZmPghTBYS"
   },
   "outputs": [],
   "source": [
    "# PREPROCESSING: remove the mean from each patch\n",
    "S = S - np.mean(S, axis=0, keepdims=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M7tfpHP0FSup"
   },
   "source": [
    "Perform dictionary learning via KSVD or MOD\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iAyxmCqpFalR"
   },
   "outputs": [],
   "source": [
    "# You can use KSVD or load a precomputed dictionary\n",
    "D = loadmat(f\"{rootfolder}/data/dict_anom_det.mat\")[\"D\"]\n",
    "\n",
    "# Or implement KSVD if you prefer:\n",
    "# D = ksvd(S, M, natom, niter_dl, S.shape[1], L)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b_Nfa8j4GbNB"
   },
   "source": [
    "Show the learned dictionary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-PcRFaqMGc84"
   },
   "outputs": [],
   "source": [
    "img_dict = get_dictionary_img(D)\n",
    "plt.imshow(img_dict, cmap=\"gray\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qhgmHEz2bJfR"
   },
   "source": [
    "## Confidence region estimation / density estimation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gUhXfzK7UOTo"
   },
   "outputs": [],
   "source": [
    "# PREPROCESSING: exclude black patches\n",
    "v = np.median(V, axis=0)\n",
    "V = V[:, v > 0.06]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nObfPNZ3UPpR"
   },
   "outputs": [],
   "source": [
    "# PREPROCESSING: remove the mean from each patch\n",
    "V = V - np.mean(V, axis=0, keepdims=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def OMP(s, D, L, tau):\n",
    "    _, N = D.shape\n",
    "    r = s.copy()  # initial residual\n",
    "    omega = []  # support set\n",
    "    x_OMP = np.zeros(N)  # final sparse code\n",
    "\n",
    "    while len(omega) < L and np.linalg.norm(r) > tau:\n",
    "        # SWEEP STEP: compute correlations between residual and dictionary atoms\n",
    "        e = np.zeros(N)\n",
    "        for j in range(N):\n",
    "            e[j] = D[:, j].T @ r\n",
    "\n",
    "        # find the column index with maximum correlation\n",
    "        jStar = np.argmax(np.abs(e))\n",
    "\n",
    "        # UPDATE support set\n",
    "        if jStar not in omega:\n",
    "            omega.append(jStar)\n",
    "\n",
    "        # update coefficients using least squares\n",
    "        D_omega = D[:, omega]\n",
    "        x_omega, _, _, _ = np.linalg.lstsq(D_omega, s, rcond=None)\n",
    "\n",
    "        # update residual\n",
    "        r = s - D_omega @ x_omega\n",
    "\n",
    "    # construct full sparse vector\n",
    "    for i, idx in enumerate(omega):\n",
    "        x_OMP[idx] = x_omega[i]\n",
    "\n",
    "    return x_OMP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "u2ne5ShRFdPY"
   },
   "outputs": [],
   "source": [
    "# sparse coding of each patch in V\n",
    "X = np.zeros((natom, npatch_region))\n",
    "for i in range(V.shape[1]):\n",
    "    # Use IRLS or OMP for sparse coding\n",
    "    X[:, i] = OMP(V[:, i], D, L, 1e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zLp7OZD2kW5X"
   },
   "outputs": [],
   "source": [
    "# computing the anomaly indicators (l1 norm, reconstruction error) for each\n",
    "# patch in V\n",
    "\n",
    "A = np.zeros(\n",
    "    (2, V.shape[1])\n",
    ")  # each column contains the values of the anomaly_scores for a patch\n",
    "\n",
    "for i in range(V.shape[1]):\n",
    "    # Anomaly indicator 1: L1 norm of sparse coefficients\n",
    "    # Anomaly indicator 2: Reconstruction error\n",
    "    reconstruction = D @ X[:, i]\n",
    "    A[0, i] = np.sum(np.abs(X[:, i]))  # L1 norm\n",
    "    A[1, i] = np.linalg.norm(V[:, i] - reconstruction) ** 2  # Reconstruction error\n",
    "\n",
    "# Estimation of mean and covariance\n",
    "mu = np.mean(A, axis=1)\n",
    "Sigma = np.cov(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xm7uYnkMFe7B"
   },
   "outputs": [],
   "source": [
    "# estimation of the threshold that gives the desired false positive rate\n",
    "# using the patches in V\n",
    "\n",
    "FPR_target = 0.1\n",
    "\n",
    "# compute the Mahalanobis distance for each indicator vector in A\n",
    "mahal_dist = np.zeros(V.shape[1])\n",
    "for i in range(A.shape[1]):\n",
    "    diff = A[:, i] - mu\n",
    "    mahal_dist[i] = np.sqrt(diff.T @ np.linalg.inv(Sigma) @ diff)\n",
    "\n",
    "# set the threshold\n",
    "threshold = np.percentile(mahal_dist, (1 - FPR_target) * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v2Bdon-dseO6"
   },
   "source": [
    "## Test phase\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_B8pgkcjsZ_l"
   },
   "outputs": [],
   "source": [
    "# load the test image\n",
    "img_test = imread(f\"{rootfolder}/data/img_anom.png\") / 255\n",
    "\n",
    "imsz = img_test.shape\n",
    "\n",
    "STEP = 7\n",
    "# initialize the estimated image\n",
    "heatmap = np.zeros_like(img)\n",
    "\n",
    "# initialize the weight matrix\n",
    "weights = np.zeros_like(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5r_jjWyFZAmR"
   },
   "outputs": [],
   "source": [
    "for i in range(0, imsz[0] - p + 1, STEP):\n",
    "    for j in range(0, imsz[1] - p + 1, STEP):\n",
    "        # extract the patch with the top left corner at pixel (i, j)\n",
    "        s = img_test[i : i + p, j : j + p].flatten()\n",
    "\n",
    "        # if the median of s is too small set the anomaly score to 0:\n",
    "        if np.median(s) <= 0.06:\n",
    "            score = 0\n",
    "        else:\n",
    "            # subtract the mean from the patch\n",
    "            s = s - np.mean(s)\n",
    "\n",
    "            # perform the sparse coding\n",
    "            x = OMP(s, D, L, 1e-6)  # or use IRLS\n",
    "\n",
    "            # compute the anomaly indicators vector\n",
    "            reconstruction = D @ x\n",
    "            a = np.array([np.sum(np.abs(x)), np.linalg.norm(s - reconstruction) ** 2])\n",
    "\n",
    "            # compute the anomaly score (Mahalanobis distance)\n",
    "            diff = a - mu\n",
    "            score = np.sqrt(diff.T @ np.linalg.inv(Sigma) @ diff)\n",
    "\n",
    "        # update the heatmap\n",
    "        heatmap[i : i + p, j : j + p] += score\n",
    "\n",
    "        # update the weight matrix\n",
    "        weights[i : i + p, j : j + p] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "V1Xe6aSIalT3"
   },
   "outputs": [],
   "source": [
    "# normalize the heatmap\n",
    "heatmap = heatmap / (weights + 1e-10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qSyo8FrHUwqs"
   },
   "outputs": [],
   "source": [
    "# plot the heatmap\n",
    "plt.imshow(heatmap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Xhsq0_XdLT7k"
   },
   "outputs": [],
   "source": [
    "# build the detection mask, that has the same size of the test image\n",
    "# each pixel in the mask has value 1 if the corresponding patch has been\n",
    "# detected as anomalous, otherwise it has value 0\n",
    "mask = (heatmap > threshold).astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "H95FcXJZMZVv"
   },
   "outputs": [],
   "source": [
    "## show the results\n",
    "plt.figure(3), plt.imshow(img_test, cmap=\"gray\"), plt.title(\"Test Image\")\n",
    "plt.figure(4), plt.imshow(mask, cmap=\"gray\"), plt.title(\"Mask\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kyRX60r7NJG0"
   },
   "outputs": [],
   "source": [
    "# combine the mask and the test image\n",
    "img_color = np.zeros([img_test.shape[0], img_test.shape[1], 3])\n",
    "img_temp = img_test.copy()\n",
    "img_temp[mask > 0] = 1\n",
    "img_color[:, :, 0] = img_temp\n",
    "img_temp = img_test.copy()\n",
    "img_temp[mask > 0] = 0\n",
    "img_color[:, :, 1] = img_temp\n",
    "img_temp = img_test.copy()\n",
    "img_temp[mask > 0] = 0\n",
    "img_color[:, :, 2] = img_temp\n",
    "\n",
    "plt.figure(5), plt.imshow(img_color), plt.title(\"Detections\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Image_processing_notes",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
