{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3Q_m4cCiDehI"
   },
   "outputs": [],
   "source": [
    "from skimage.io import imread\n",
    "import numpy as np\n",
    "from scipy.io import loadmat\n",
    "import scipy as sp\n",
    "from matplotlib import pyplot as plt\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dCOIY19mDxRR"
   },
   "outputs": [],
   "source": [
    "rootfolder = \"..\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-NB1-MvmFsot"
   },
   "source": [
    "Useful function for plot the dictionary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Rey7kIlUF22r"
   },
   "outputs": [],
   "source": [
    "def get_dictionary_img(D):\n",
    "    M, N = D.shape\n",
    "    p = int(round(np.sqrt(M)))\n",
    "    nnn = int(np.ceil(np.sqrt(N)))\n",
    "    bound = 2\n",
    "    img = np.ones((nnn * p + bound * (nnn - 1), nnn * p + bound * (nnn - 1)))\n",
    "    for i in range(N):\n",
    "        m = np.mod(i, nnn)\n",
    "        n = int((i - m) / nnn)\n",
    "        m = m * p + bound * m\n",
    "        n = n * p + bound * n\n",
    "        atom = D[:, i].reshape((p, p))\n",
    "        if atom.min() < atom.max():\n",
    "            atom = (atom - atom.min()) / (atom.max() - atom.min())\n",
    "        img[m : m + p, n : n + p] = atom\n",
    "\n",
    "    return img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QBmBkQRSEBFw"
   },
   "source": [
    "Set all the parameters for the anomaly detection\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SOQHJpvHD8v3"
   },
   "outputs": [],
   "source": [
    "# patch size (tha patch is square)\n",
    "p = 15\n",
    "\n",
    "# number of patches in the training set for dictionary learning\n",
    "npatch_dictionary = 10000\n",
    "\n",
    "# number of patches to estimate the confidence region\n",
    "npatch_region = 1000\n",
    "\n",
    "# parameters for the dictionary learning using the KSVD\n",
    "niter_dl = 10\n",
    "natom = int(np.round(p**2 * 1.5))\n",
    "L = 4\n",
    "\n",
    "# regularization parameters for the l1 sparse coding\n",
    "lmbda = 0.18"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "acWegV3eIXmN"
   },
   "source": [
    "## Construct the training and validation sets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "g32ca1edFYgA"
   },
   "outputs": [],
   "source": [
    "# load the training image and rescale it in [0,1]\n",
    "img = imread(f\"{rootfolder}/data/img_normal.png\") / 255\n",
    "\n",
    "# extract random patches from the image and store them in a matrices S, V\n",
    "# S =\n",
    "# V ="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y0JL30HzE76w"
   },
   "source": [
    "## Dictionary Learning\n",
    "\n",
    "Perform preprocessing on the patches in $S$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ppQOaVIlTjGW"
   },
   "outputs": [],
   "source": [
    "# PREPROCESSING: exclude black patches from S\n",
    "v = np.median(S, axis=0)\n",
    "S = S[:, v > 0.06]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qemZmPghTBYS"
   },
   "outputs": [],
   "source": [
    "# PREPROCESSING: remove the mean from each patch\n",
    "# S =\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M7tfpHP0FSup"
   },
   "source": [
    "Perform dictionary learning via KSVD or MOD\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iAyxmCqpFalR"
   },
   "outputs": [],
   "source": [
    "# D =\n",
    "\n",
    "# or load a precomputed dictionary\n",
    "# D = loadmat(f'{rootfolder}/data/dict_anom_det.mat')['D']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b_Nfa8j4GbNB"
   },
   "source": [
    "Show the learned dictionary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-PcRFaqMGc84"
   },
   "outputs": [],
   "source": [
    "img_dict = get_dictionary_img(D)\n",
    "plt.imshow(img_dict, cmap=\"gray\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qhgmHEz2bJfR"
   },
   "source": [
    "## Confidence region estimation / density estimation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gUhXfzK7UOTo"
   },
   "outputs": [],
   "source": [
    "# PREPROCESSING: exclude black patches\n",
    "v = np.median(V, axis=0)\n",
    "V = V[:, v > 0.06]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nObfPNZ3UPpR"
   },
   "outputs": [],
   "source": [
    "# PREPROCESSING: remove the mean from each patch\n",
    "V ="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "u2ne5ShRFdPY"
   },
   "outputs": [],
   "source": [
    "# sparse coding of each patch in V\n",
    "X = np.zeros((natom, npatch_region))\n",
    "for i in range(V.shape[1]):\n",
    "#    X[:, i] =\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zLp7OZD2kW5X"
   },
   "outputs": [],
   "source": [
    "# computing the anomaly indicators (l1 norm, reconstruction error) for each\n",
    "# patch in V\n",
    "\n",
    "A = np.zeros((2, V.shape[1]))     # each column contains the values of the anomaly_scores for a patch\n",
    "\n",
    "for i in range(V.shape[1]):\n",
    "#    A[:,i] =\n",
    "\n",
    "# Estimation of mean and covariance\n",
    "#mu =\n",
    "#Sigma =\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xm7uYnkMFe7B"
   },
   "outputs": [],
   "source": [
    "# estimation of the threshold that gives the desired false positive rate\n",
    "# using the patches in V\n",
    "\n",
    "FPR_target = 0.1\n",
    "\n",
    "# compute the Mahalanobis distance for each indicator vector in A\n",
    "mahal_dist = np.zeros(V.shape[1])\n",
    "for i in range(A.shape[1]):\n",
    "#    mahal_dist[i] =\n",
    "\n",
    "# set the threshold\n",
    "#threshold =\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v2Bdon-dseO6"
   },
   "source": [
    "## Test phase\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_B8pgkcjsZ_l"
   },
   "outputs": [],
   "source": [
    "# load the test image\n",
    "img_test = imread(f\"{rootfolder}/data/img_anom.png\") / 255\n",
    "\n",
    "imsz = img_test.shape\n",
    "\n",
    "STEP = 7\n",
    "# initialize the estimated image\n",
    "heatmap = np.zeros_like(img)\n",
    "\n",
    "# initialize the weight matrix\n",
    "weights = np.zeros_like(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5r_jjWyFZAmR"
   },
   "outputs": [],
   "source": [
    "for i in range(0, imsz[0] - p + 1, STEP):\n",
    "    for j in range(0, imsz[1] - p + 1, STEP):\n",
    "        # extrach the patch with the top left corner at pixel (ii, jj)\n",
    "        #s =\n",
    "\n",
    "        # if the median of s is to small set the anomaly score to 0:\n",
    "        if np.median(s) <= 0.06:\n",
    "            score = 0\n",
    "        else:\n",
    "            # subratct the mean to the patch\n",
    "            #s =\n",
    "\n",
    "            # perform the sparse coding\n",
    "            #x =\n",
    "\n",
    "            # compute the anomaly indicators vector\n",
    "            #a =\n",
    "\n",
    "            # compute the anomaly score\n",
    "            #score =\n",
    "\n",
    "        # update the heatmap\n",
    "        #heatmap\n",
    "\n",
    "        # update the weight matrix\n",
    "        #weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "V1Xe6aSIalT3"
   },
   "outputs": [],
   "source": [
    "# normalize the heatmap\n",
    "# heatmap ="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qSyo8FrHUwqs"
   },
   "outputs": [],
   "source": [
    "# plot the heatmap\n",
    "plt.imshow(heatmap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Xhsq0_XdLT7k"
   },
   "outputs": [],
   "source": [
    "# build the detection mask, that has the same size of the test image\n",
    "# each pixel in the mask has value 1 if the corresponding patch has been\n",
    "# detected as anomalous, otherwise it has value 0\n",
    "# mask =\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "H95FcXJZMZVv"
   },
   "outputs": [],
   "source": [
    "## show the results\n",
    "plt.figure(3), plt.imshow(img_test, cmap=\"gray\"), plt.title(\"Test Image\")\n",
    "plt.figure(4), plt.imshow(mask, cmap=\"gray\"), plt.title(\"Mask\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kyRX60r7NJG0"
   },
   "outputs": [],
   "source": [
    "# combine the mask and the test image\n",
    "img_color = np.zeros([img_test.shape[0], img_test.shape[1], 3])\n",
    "img_temp = img_test.copy()\n",
    "img_temp[mask > 0] = 1\n",
    "img_color[:, :, 0] = img_temp\n",
    "img_temp = img_test.copy()\n",
    "img_temp[mask > 0] = 0\n",
    "img_color[:, :, 1] = img_temp\n",
    "img_temp = img_test.copy()\n",
    "img_temp[mask > 0] = 0\n",
    "img_color[:, :, 2] = img_temp\n",
    "\n",
    "plt.figure(5), plt.imshow(img_color), plt.title(\"Detections\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Image_processing_notes",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
