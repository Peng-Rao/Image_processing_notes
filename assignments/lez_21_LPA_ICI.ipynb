{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "quAd5kUJXS2h"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from scipy.signal import convolve\n",
    "\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format='retina'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n9KEr7OUZ00l"
   },
   "source": [
    "Define the function to compute the kernel given the weights and the degree of the polynomial\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ig2s_rwtZx6P"
   },
   "outputs": [],
   "source": [
    "def compute_LPA_kernel(w, N):\n",
    "    M = len(w)\n",
    "\n",
    "    # Create normalized time vector\n",
    "    t = np.arange(M) / (M - 1)\n",
    "\n",
    "    # Build polynomial basis matrix\n",
    "    T = np.zeros((M, N + 1))\n",
    "    for i in range(N + 1):\n",
    "        T[:, i] = t**i\n",
    "\n",
    "    # Handle zero weights\n",
    "    winv = np.zeros_like(w)\n",
    "    winv[w != 0] = 1 / w[w != 0]\n",
    "    # set to zero weights that are inf\n",
    "    winv[np.isinf(winv)] = 0\n",
    "\n",
    "    # Create weight matrices\n",
    "    W = np.diag(w)\n",
    "    Winv = np.diag(winv)\n",
    "\n",
    "    # QR decomposition of weighted T\n",
    "    Q, R = np.linalg.qr(W @ T)\n",
    "\n",
    "    # Compute Qtilde\n",
    "    Qtilde = Winv @ Q\n",
    "\n",
    "    # Compute W²Qtilde\n",
    "    W2Qtilde = W @ W @ Qtilde\n",
    "\n",
    "    # Select central row\n",
    "    row = int((M - 1) / 2)\n",
    "\n",
    "    # Compute kernel (using first column for constant term)\n",
    "    g = W2Qtilde[row, 0] * Qtilde[:, 0]\n",
    "\n",
    "    # Flip for convolution\n",
    "    g = np.flip(g)\n",
    "\n",
    "    return g\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "34S3SYMckqtD"
   },
   "source": [
    "## LPA-ICI\n",
    "\n",
    "Set the LPA-ICI parameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yYPg7PvuaT9Y"
   },
   "outputs": [],
   "source": [
    "# maximum degree of polynomial used for fitting\n",
    "N = 5\n",
    "\n",
    "# parameter for the confidence intervals in the ICI rule\n",
    "Gamma = 2\n",
    "\n",
    "# Set all the scale values\n",
    "hmax = 51\n",
    "all_h = np.arange(1, hmax + 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tVZHR5NbaKNZ"
   },
   "source": [
    "Generate the signal\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IlgyMA_SaMZz"
   },
   "outputs": [],
   "source": [
    "LENGTH = 1000\n",
    "\n",
    "ty = np.linspace(0, 1, LENGTH)\n",
    "y = np.sin(2 / (ty + 0.05))\n",
    "\n",
    "#  noise standard deviation\n",
    "sigma = 0.2\n",
    "\n",
    "# noisy signal\n",
    "s = y + sigma * np.random.normal(size=LENGTH)\n",
    "\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(ty, s, \"r.\")\n",
    "plt.plot(ty, y, \"k--\", linewidth=2)\n",
    "plt.grid()\n",
    "plt.legend([\"noisy\", \"original\"])\n",
    "plt.title(\"Input Signal\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DMZBeqQtcunJ"
   },
   "source": [
    "Generate the LPA kernels for all the scale. Use centered weights.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qUUogpW_qmdi"
   },
   "outputs": [],
   "source": [
    "all_g = []\n",
    "for i in range(len(all_h)):\n",
    "    h = all_h[i]\n",
    "    # define the weights for the scale h (symmetric, centered)\n",
    "    w = np.zeros(2 * hmax + 1)\n",
    "    w[hmax - h : hmax + h + 1] = 1\n",
    "\n",
    "    # compute and store the kernel g\n",
    "    g = compute_LPA_kernel(w, N)\n",
    "\n",
    "    all_g.append(g)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fBP6KA27s_kF"
   },
   "source": [
    "Initialize all the variables for the ICI rule\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "23d65QPcrRso"
   },
   "outputs": [],
   "source": [
    "# initialize the estimate\n",
    "yhat = np.zeros_like(s)\n",
    "\n",
    "# initialize the vector containing the best scale for each sample\n",
    "best_scale = np.ones(LENGTH, dtype=int)\n",
    "\n",
    "# initialize the lower and upper bound vectors\n",
    "lower_bounds = -np.inf * np.ones(LENGTH)\n",
    "upper_bounds = np.inf * np.ones(LENGTH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AaCGFuCrlbGI"
   },
   "source": [
    "Loop over all the scales\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5nuG5GihlgQ3"
   },
   "outputs": [],
   "source": [
    "for i, h in enumerate(all_h):\n",
    "    g = all_g[i]\n",
    "\n",
    "    # compute the estimate for the scale h\n",
    "    yhat_h = convolve(s, g, mode=\"same\")\n",
    "\n",
    "    # compute the variance of the estimate\n",
    "    var_h = sigma**2 * convolve(np.ones_like(s), g**2, mode=\"same\")\n",
    "\n",
    "    # compute the lower and upper bound of the confidence interval for the scale h\n",
    "    lb = yhat_h - Gamma * np.sqrt(var_h)\n",
    "    ub = yhat_h + Gamma * np.sqrt(var_h)\n",
    "\n",
    "    # update the lower and upper bounds (intersection)\n",
    "    lower_bounds = np.maximum(lower_bounds, lb)\n",
    "    upper_bounds = np.minimum(upper_bounds, ub)\n",
    "\n",
    "    # identify for which samples h is the best scale according to the\n",
    "    # ICI rule and update the best_scale vector accordingly\n",
    "    valid_ici = lower_bounds <= upper_bounds\n",
    "    best_scale[valid_ici] = h\n",
    "\n",
    "    # update the estimate\n",
    "    yhat[valid_ici] = yhat_h[valid_ici]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dVx-E5q8tCs3"
   },
   "source": [
    "Use the best scale for each sample to compute the final estimates\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GHi9A8uXqryE"
   },
   "outputs": [],
   "source": [
    "yhat_final = yhat.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tBJUSUuqdp7v"
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(2, 1, figsize=(12, 7))\n",
    "ax[0].plot(ty, s, \"r.\")\n",
    "ax[0].plot(ty, y, \"k--\", linewidth=3)\n",
    "ax[0].plot(ty, yhat_final, \"m-\", linewidth=3, color=\"blue\")\n",
    "ax[0].grid()\n",
    "ax[0].legend([\"noisy\", \"original\", \"LPA-ICI estimate\"])\n",
    "ax[0].set_title(f\"N = {N:d}\")\n",
    "\n",
    "ax[1].plot(ty, best_scale, \"r.\")\n",
    "ax[1].set_title(\"Scale selected by ICI rule\")\n",
    "ax[1].grid()\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "increase the degree of the polynomial N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_lpa_ici(s, N, Gamma, hmax, sigma):\n",
    "    \"\"\"\n",
    "    Run LPA-ICI algorithm with given parameters\n",
    "    \"\"\"\n",
    "    LENGTH = len(s)\n",
    "    all_h = np.arange(1, hmax + 1)\n",
    "\n",
    "    # Generate the LPA kernels for all scales\n",
    "    all_g = []\n",
    "    for i in range(len(all_h)):\n",
    "        h = all_h[i]\n",
    "        # define the weights for the scale h (symmetric, centered)\n",
    "        w = np.zeros(2 * hmax + 1)\n",
    "        w[hmax - h : hmax + h + 1] = 1\n",
    "\n",
    "        # compute and store the kernel g\n",
    "        g = compute_LPA_kernel(w, N)\n",
    "        all_g.append(g)\n",
    "\n",
    "    # Initialize all the variables for the ICI rule\n",
    "    yhat = np.zeros_like(s)\n",
    "    best_scale = np.ones(LENGTH, dtype=int)\n",
    "    lower_bounds = -np.inf * np.ones(LENGTH)\n",
    "    upper_bounds = np.inf * np.ones(LENGTH)\n",
    "\n",
    "    # Loop over all the scales\n",
    "    for i, h in enumerate(all_h):\n",
    "        g = all_g[i]\n",
    "\n",
    "        # compute the estimate for the scale h\n",
    "        yhat_h = convolve(s, g, mode=\"same\")\n",
    "\n",
    "        # compute the variance of the estimate\n",
    "        var_h = sigma**2 * convolve(np.ones_like(s), g**2, mode=\"same\")\n",
    "\n",
    "        # compute the lower and upper bound of the confidence interval\n",
    "        lb = yhat_h - Gamma * np.sqrt(var_h)\n",
    "        ub = yhat_h + Gamma * np.sqrt(var_h)\n",
    "\n",
    "        # update the lower and upper bounds (intersection)\n",
    "        lower_bounds = np.maximum(lower_bounds, lb)\n",
    "        upper_bounds = np.minimum(upper_bounds, ub)\n",
    "\n",
    "        # identify for which samples h is the best scale\n",
    "        valid_ici = lower_bounds <= upper_bounds\n",
    "        best_scale[valid_ici] = h\n",
    "\n",
    "        # update the estimate\n",
    "        yhat[valid_ici] = yhat_h[valid_ici]\n",
    "\n",
    "    return yhat, best_scale\n",
    "\n",
    "\n",
    "# Generate the signal\n",
    "LENGTH = 1000\n",
    "ty = np.linspace(0, 1, LENGTH)\n",
    "y = np.sin(2 / (ty + 0.05))\n",
    "\n",
    "# noise standard deviation\n",
    "sigma = 0.2\n",
    "\n",
    "# noisy signal\n",
    "s = y + sigma * np.random.normal(size=LENGTH)\n",
    "\n",
    "# Experiment with different polynomial degrees\n",
    "N_values = [1, 3, 5, 7, 9]\n",
    "Gamma = 2\n",
    "hmax = 51\n",
    "\n",
    "# Store results for different N values\n",
    "results = {}\n",
    "\n",
    "for N in N_values:\n",
    "    yhat, best_scale = run_lpa_ici(s, N, Gamma, hmax, sigma)\n",
    "\n",
    "    # Compute bias and variance estimates\n",
    "    # For multiple realizations, we'd need to run this many times\n",
    "    # Here we approximate: bias ≈ |E[yhat] - y| ≈ |yhat - y| for visualization\n",
    "    bias = np.abs(yhat - y)\n",
    "\n",
    "    # Estimate local variance by looking at local variations\n",
    "    window = 20\n",
    "    local_var = np.zeros_like(yhat)\n",
    "    for i in range(window // 2, LENGTH - window // 2):\n",
    "        local_segment = yhat[i - window // 2 : i + window // 2]\n",
    "        local_var[i] = np.var(local_segment)\n",
    "\n",
    "    results[N] = {\n",
    "        \"yhat\": yhat,\n",
    "        \"best_scale\": best_scale,\n",
    "        \"bias\": bias,\n",
    "        \"local_var\": local_var,\n",
    "    }\n",
    "\n",
    "# Create comprehensive visualization\n",
    "fig = plt.figure(figsize=(15, 12))\n",
    "\n",
    "# Plot 1: Original signal and estimates for different N\n",
    "ax1 = plt.subplot(3, 2, 1)\n",
    "ax1.plot(ty, s, \"r.\", alpha=0.3, markersize=2, label=\"noisy\")\n",
    "ax1.plot(ty, y, \"k--\", linewidth=2, label=\"original\")\n",
    "for N in N_values[::2]:  # Plot every other N to avoid clutter\n",
    "    ax1.plot(ty, results[N][\"yhat\"], linewidth=2, alpha=0.7, label=f\"N={N}\")\n",
    "ax1.grid(True)\n",
    "ax1.legend()\n",
    "ax1.set_title(\"LPA-ICI Estimates for Different Polynomial Degrees\")\n",
    "ax1.set_xlabel(\"Time\")\n",
    "ax1.set_ylabel(\"Signal Value\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kMvwf8ram7Lf"
   },
   "source": [
    "## LPA-ICI with Aggregation\n",
    "\n",
    "Set the parameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZtsjdjSWnENQ"
   },
   "outputs": [],
   "source": [
    "# maximum degree of polynomial used for fitting\n",
    "N = 1\n",
    "\n",
    "# parameter for the confidence intervals in the ICI rule\n",
    "Gamma = 2\n",
    "\n",
    "# Set all the scale values\n",
    "hmax = 51\n",
    "all_h = np.arange(1, hmax + 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YXG1JzYknKLl"
   },
   "source": [
    "Generate synthetic signal signal\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Vfp46DzKnMMq"
   },
   "outputs": [],
   "source": [
    "LENGTH = 1000\n",
    "ty = np.linspace(0, 1, LENGTH)\n",
    "y = 8 * ty**2 - 2 * ty + 2\n",
    "y[ty > 0.5] = y[ty > 0.5] + 7\n",
    "\n",
    "#  noise standard deviation\n",
    "sigma = 0.3\n",
    "\n",
    "# noisy signal\n",
    "s = y + sigma * np.random.normal(size=LENGTH)\n",
    "\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(ty, s, \"r.\")\n",
    "plt.plot(ty, y, \"k--\", linewidth=2)\n",
    "plt.grid()\n",
    "plt.legend([\"noisy\", \"original\"])\n",
    "plt.title(\"Input Signal\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tfg3XYvInP9p"
   },
   "source": [
    "Generate the LPA kernels for all the scale for both left and right windows\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oy5unf_onSWi"
   },
   "outputs": [],
   "source": [
    "all_g_left = []\n",
    "all_g_right = []\n",
    "\n",
    "for i, h in enumerate(all_h):\n",
    "    # define the weights for the scale h (left)\n",
    "    w = np.zeros(2 * hmax + 1)\n",
    "    w[hmax - h : hmax + 1] = 1  # left window\n",
    "    g_left = compute_LPA_kernel(w, N)\n",
    "    all_g_left.append(g_left)\n",
    "\n",
    "    # define the weights for the scale h (right)\n",
    "    w = np.zeros(2 * hmax + 1)\n",
    "    w[hmax : hmax + h + 1] = 1  # right window\n",
    "    g_right = compute_LPA_kernel(w, N)\n",
    "    all_g_right.append(g_right)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OPapyBO_oFEj"
   },
   "source": [
    "Use the LPA-ICI to compute the estimate based on the **left** kernels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pNrjeHXOoRbo"
   },
   "outputs": [],
   "source": [
    "# initialize the left estimate\n",
    "yhat_left = np.zeros_like(s)\n",
    "\n",
    "# initialize the lower and upper bound vectors\n",
    "lower_bounds = -np.inf * np.ones(LENGTH)\n",
    "upper_bounds = np.inf * np.ones(LENGTH)\n",
    "\n",
    "# intialize the vector containing the variance of the estimator for each sample\n",
    "var_left = np.zeros_like(s)\n",
    "\n",
    "for i, h in enumerate(all_h):\n",
    "    g = all_g_left[i]\n",
    "\n",
    "    # compute the estimate for the scale h\n",
    "    yhat_h = convolve(s, g, mode=\"same\")\n",
    "\n",
    "    # compute the variance of the estimate\n",
    "    var_h = sigma**2 * convolve(np.ones_like(s), g**2, mode=\"same\")\n",
    "\n",
    "    # compute the lower and upper bound of the confidence interval for the scale h\n",
    "    lb = yhat_h - Gamma * np.sqrt(var_h)\n",
    "    ub = yhat_h + Gamma * np.sqrt(var_h)\n",
    "\n",
    "    # update the lower and upper bounds\n",
    "    lower_bounds = np.maximum(lower_bounds, lb)\n",
    "    upper_bounds = np.minimum(upper_bounds, ub)\n",
    "\n",
    "    # identify valid ICI points\n",
    "    valid_ici = lower_bounds <= upper_bounds\n",
    "\n",
    "    # update the estimate\n",
    "    yhat_left[valid_ici] = yhat_h[valid_ici]\n",
    "\n",
    "    # update the variance\n",
    "    var_left[valid_ici] = var_h[valid_ici]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iai8z6xLolaw"
   },
   "source": [
    "Use the LPA-ICI to compute the estimate based on the **right** kernels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "K7vfQJb9ola4"
   },
   "outputs": [],
   "source": [
    "yhat_right = np.zeros_like(s)\n",
    "# initialize the lower and upper bound vectors\n",
    "lower_bounds = -np.inf * np.ones(LENGTH)\n",
    "upper_bounds = np.inf * np.ones(LENGTH)\n",
    "\n",
    "# intialize the vector containing the variance of the estimator for each sample\n",
    "var_right = np.zeros_like(s)\n",
    "\n",
    "for i, h in enumerate(all_h):\n",
    "    g = all_g_right[i]  # Fixed: was using all_g_left[i]\n",
    "\n",
    "    # compute the estimate for the scale h\n",
    "    yhat_h = convolve(s, g, mode=\"same\")\n",
    "\n",
    "    # compute the variance of the estimate\n",
    "    var_h = sigma**2 * convolve(np.ones_like(s), g**2, mode=\"same\")\n",
    "\n",
    "    # compute the lower and upper bound of the confidence interval for the scale h\n",
    "    lb = yhat_h - Gamma * np.sqrt(var_h)\n",
    "    ub = yhat_h + Gamma * np.sqrt(var_h)\n",
    "\n",
    "    # update the lower and upper bounds\n",
    "    lower_bounds = np.maximum(lower_bounds, lb)\n",
    "    upper_bounds = np.minimum(upper_bounds, ub)\n",
    "\n",
    "    # identify valid ICI points\n",
    "    valid_ici = lower_bounds <= upper_bounds\n",
    "\n",
    "    # update the estimate\n",
    "    yhat_right[valid_ici] = yhat_h[valid_ici]\n",
    "\n",
    "    # update the variance\n",
    "    var_right[valid_ici] = var_h[valid_ici]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CSVwLUp8pFm4"
   },
   "source": [
    "Perform the aggregation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Jpw96MPppJL6"
   },
   "outputs": [],
   "source": [
    "weight_left = 1 / (var_left + 1e-10)  # add small epsilon to avoid division by zero\n",
    "weight_right = 1 / (var_right + 1e-10)\n",
    "total_weight = weight_left + weight_right\n",
    "\n",
    "yhat_aggr = (weight_left * yhat_left + weight_right * yhat_right) / total_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UlIPoQuvqH5v"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 10))\n",
    "plt.plot(ty, s, \"r.\")\n",
    "plt.plot(ty, y, \"k--\", linewidth=3)\n",
    "plt.plot(ty, yhat_right, \"m-\", linewidth=3)\n",
    "plt.plot(ty, yhat_left, \"g-\", linewidth=3)\n",
    "plt.plot(ty, yhat_aggr, \"b-\", linewidth=3)\n",
    "plt.grid()\n",
    "plt.legend(\n",
    "    [\"noisy\", \"original\", \"right estimate\", \"left estimate\", \"aggregated estimate\"]\n",
    ")\n",
    "plt.title(f\"N = {N:d}\")\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyNb3zFnnetv90zpaqEpgLju",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Image_processing_notes",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
