{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "quAd5kUJXS2h"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from scipy.signal import convolve\n",
    "\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format='retina'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n9KEr7OUZ00l"
   },
   "source": [
    "Define the function to compute the kernel given the weights and the degree of the polynomial\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ig2s_rwtZx6P"
   },
   "outputs": [],
   "source": [
    "def compute_LPA_kernel(w, N):\n",
    "    \"\"\"\n",
    "    Compute the LPA (Local Polynomial Approximation) kernel for a given weight vector and polynomial degree.\n",
    "\n",
    "    Parameters:\n",
    "    - w: numpy array of weights (length M), centered at position 0\n",
    "    - N: degree of the polynomial approximation\n",
    "\n",
    "    Returns:\n",
    "    - g: numpy array representing the LPA kernel (same length as w)\n",
    "    \"\"\"\n",
    "    M = len(w)\n",
    "\n",
    "    # generate the inverse of weights\n",
    "    winv = np.zeros_like(w)\n",
    "    winv[w != 0] = 1 / w[w != 0]\n",
    "\n",
    "    # set to zero weights that are inf\n",
    "    winv[np.isinf(winv)] = 0\n",
    "\n",
    "    # define the weight matrices\n",
    "    W = np.diag(w)\n",
    "    Winv = np.diag(winv)\n",
    "\n",
    "    # Define the matrix T containing polynomials sampled over the window\n",
    "    t = np.arange(M) / (M - 1)  # normalized time over the window [0, 1]\n",
    "    T = np.zeros((M, N + 1))\n",
    "    for i in range(N + 1):\n",
    "        T[:, i] = t**i\n",
    "\n",
    "    # compute the qr decomposition of WT\n",
    "    Q, R = np.linalg.qr(W @ T)\n",
    "\n",
    "    # define Qtilde\n",
    "    Qtilde = Winv @ Q\n",
    "\n",
    "    # adjust Qtilde with the weights matrix squared\n",
    "    W2Qtilde = W @ W @ Qtilde\n",
    "\n",
    "    # select the central row of W2Qtilde\n",
    "    row = int((M - 1) / 2)\n",
    "\n",
    "    # compute the kernel (first column of Q corresponds to the constant term)\n",
    "    g = W2Qtilde[row, 0] * Qtilde[:, 0]\n",
    "\n",
    "    # flipping, since it is used in convolution\n",
    "    g = np.flip(g)\n",
    "\n",
    "    return g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lpa_kernel(support, degree, weights=None):\n",
    "    \"\"\"\n",
    "    Compute LPA convolution kernel\n",
    "\n",
    "    Parameters:\n",
    "    support: array of spatial support locations\n",
    "    degree: polynomial degree\n",
    "    weights: optional weight vector\n",
    "\n",
    "    Returns:\n",
    "    h: convolution kernel\n",
    "    \"\"\"\n",
    "\n",
    "    M = len(support)\n",
    "    L = degree\n",
    "\n",
    "    # Design matrix\n",
    "    T = np.zeros((M, L + 1))\n",
    "    for i in range(M):\n",
    "        for j in range(L + 1):\n",
    "            T[i, j] = support[i] ** j\n",
    "\n",
    "    # Apply weights if provided\n",
    "    if weights is not None:\n",
    "        W = np.diag(weights)\n",
    "        T = W @ T\n",
    "\n",
    "    # QR decomposition\n",
    "    Q, R = qr(T, mode=\"economic\")\n",
    "\n",
    "    # Central index\n",
    "    ic = M // 2\n",
    "\n",
    "    # Compute kernel\n",
    "    e_ic = np.zeros(M)\n",
    "    e_ic[ic] = 1\n",
    "    h = Q @ Q.T @ e_ic\n",
    "\n",
    "    return h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "34S3SYMckqtD"
   },
   "source": [
    "## LPA-ICI\n",
    "\n",
    "Set the LPA-ICI parameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yYPg7PvuaT9Y"
   },
   "outputs": [],
   "source": [
    "# maximum degree of polynomial used for fitting\n",
    "N = 5\n",
    "\n",
    "# parameter for the confidence intervals in the ICI rule\n",
    "Gamma = 2\n",
    "\n",
    "# Set all the scale values\n",
    "hmax = 51\n",
    "all_h = np.arange(1, hmax + 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tVZHR5NbaKNZ"
   },
   "source": [
    "Generate the signal\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IlgyMA_SaMZz"
   },
   "outputs": [],
   "source": [
    "LENGTH = 1000\n",
    "\n",
    "ty = np.linspace(0, 1, LENGTH)\n",
    "y = np.sin(2 / (ty + 0.05))\n",
    "\n",
    "#  noise standard deviation\n",
    "sigma = 0.2\n",
    "\n",
    "# noisy signal\n",
    "s = y + sigma * np.random.normal(size=LENGTH)\n",
    "\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(ty, s, \"r.\")\n",
    "plt.plot(ty, y, \"k--\", linewidth=2)\n",
    "plt.grid()\n",
    "plt.legend([\"noisy\", \"original\"])\n",
    "plt.title(\"Input Signal\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DMZBeqQtcunJ"
   },
   "source": [
    "Generate the LPA kernels for all the scale. Use centered weights.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qUUogpW_qmdi"
   },
   "outputs": [],
   "source": [
    "all_g = []\n",
    "for i in range(len(all_h)):\n",
    "    h = all_h[i]\n",
    "    # define the weights for the scale h (symmetric, centered)\n",
    "    w = np.zeros(2 * hmax + 1)\n",
    "    w[hmax - h : hmax + h + 1] = 1\n",
    "\n",
    "    # compute and store the kernel g\n",
    "    g = compute_LPA_kernel(w, N)\n",
    "\n",
    "    all_g.append(g)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fBP6KA27s_kF"
   },
   "source": [
    "Initialize all the variables for the ICI rule\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "23d65QPcrRso"
   },
   "outputs": [],
   "source": [
    "# initialize the estimate\n",
    "yhat = np.zeros_like(s)\n",
    "\n",
    "# initialize the vector containing the best scale for each sample\n",
    "best_scale = np.ones(LENGTH, dtype=int)\n",
    "\n",
    "# initialize the lower and upper bound vectors\n",
    "lower_bounds = -np.inf * np.ones(LENGTH)\n",
    "upper_bounds = np.inf * np.ones(LENGTH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AaCGFuCrlbGI"
   },
   "source": [
    "Loop over all the scales\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5nuG5GihlgQ3"
   },
   "outputs": [],
   "source": [
    "for i, h in enumerate(all_h):\n",
    "    g = all_g[i]\n",
    "\n",
    "    # compute the estimate for the scale h\n",
    "    yhat_h = convolve(s, g, mode=\"same\")\n",
    "\n",
    "    # compute the variance of the estimate\n",
    "    var_h = sigma**2 * convolve(np.ones_like(s), g**2, mode=\"same\")\n",
    "\n",
    "    # compute the lower and upper bound of the confidence interval for the scale h\n",
    "    lb = yhat_h - Gamma * np.sqrt(var_h)\n",
    "    ub = yhat_h + Gamma * np.sqrt(var_h)\n",
    "\n",
    "    # update the lower and upper bounds (intersection)\n",
    "    lower_bounds = np.maximum(lower_bounds, lb)\n",
    "    upper_bounds = np.minimum(upper_bounds, ub)\n",
    "\n",
    "    # identify for which samples h is the best scale according to the\n",
    "    # ICI rule and update the best_scale vector accordingly\n",
    "    valid_ici = lower_bounds <= upper_bounds\n",
    "    best_scale[valid_ici] = h\n",
    "\n",
    "    # update the estimate\n",
    "    yhat[valid_ici] = yhat_h[valid_ici]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dVx-E5q8tCs3"
   },
   "source": [
    "Use the best scale for each sample to compute the final estimates\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GHi9A8uXqryE"
   },
   "outputs": [],
   "source": [
    "yhat_final = yhat.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tBJUSUuqdp7v"
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(2, 1, figsize=(12, 7))\n",
    "ax[0].plot(ty, s, \"r.\")\n",
    "ax[0].plot(ty, y, \"k--\", linewidth=3)\n",
    "ax[0].plot(ty, yhat_final, \"m-\", linewidth=3, color=\"blue\")\n",
    "ax[0].grid()\n",
    "ax[0].legend([\"noisy\", \"original\", \"LPA-ICI estimate\"])\n",
    "ax[0].set_title(f\"N = {N:d}\")\n",
    "\n",
    "ax[1].plot(ty, best_scale, \"r.\")\n",
    "ax[1].set_title(\"Scale selected by ICI rule\")\n",
    "ax[1].grid()\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kMvwf8ram7Lf"
   },
   "source": [
    "## LPA-ICI with Aggregation\n",
    "\n",
    "Set the parameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZtsjdjSWnENQ"
   },
   "outputs": [],
   "source": [
    "# maximum degree of polynomial used for fitting\n",
    "N = 1\n",
    "\n",
    "# parameter for the confidence intervals in the ICI rule\n",
    "Gamma = 2\n",
    "\n",
    "# Set all the scale values\n",
    "hmax = 51\n",
    "all_h = np.arange(1, hmax + 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YXG1JzYknKLl"
   },
   "source": [
    "Generate synthetic signal signal\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Vfp46DzKnMMq"
   },
   "outputs": [],
   "source": [
    "LENGTH = 1000\n",
    "ty = np.linspace(0, 1, LENGTH)\n",
    "y = 8 * ty**2 - 2 * ty + 2\n",
    "y[ty > 0.5] = y[ty > 0.5] + 7\n",
    "\n",
    "#  noise standard deviation\n",
    "sigma = 0.3\n",
    "\n",
    "# noisy signal\n",
    "s = y + sigma * np.random.normal(size=LENGTH)\n",
    "\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(ty, s, \"r.\")\n",
    "plt.plot(ty, y, \"k--\", linewidth=2)\n",
    "plt.grid()\n",
    "plt.legend([\"noisy\", \"original\"])\n",
    "plt.title(\"Input Signal\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tfg3XYvInP9p"
   },
   "source": [
    "Generate the LPA kernels for all the scale for both left and right windows\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oy5unf_onSWi"
   },
   "outputs": [],
   "source": [
    "all_g_left = []\n",
    "all_g_right = []\n",
    "\n",
    "for i, h in enumerate(all_h):\n",
    "    # define the weights for the scale h (left)\n",
    "    w = np.zeros(2 * hmax + 1)\n",
    "    w[hmax - h : hmax + 1] = 1  # left window\n",
    "    g_left = compute_LPA_kernel(w, N)\n",
    "    all_g_left.append(g_left)\n",
    "\n",
    "    # define the weights for the scale h (right)\n",
    "    w = np.zeros(2 * hmax + 1)\n",
    "    w[hmax : hmax + h + 1] = 1  # right window\n",
    "    g_right = compute_LPA_kernel(w, N)\n",
    "    all_g_right.append(g_right)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OPapyBO_oFEj"
   },
   "source": [
    "Use the LPA-ICI to compute the estimate based on the **left** kernels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pNrjeHXOoRbo"
   },
   "outputs": [],
   "source": [
    "# initialize the left estimate\n",
    "yhat_left = np.zeros_like(s)\n",
    "\n",
    "# initialize the lower and upper bound vectors\n",
    "lower_bounds = -np.inf * np.ones(LENGTH)\n",
    "upper_bounds = np.inf * np.ones(LENGTH)\n",
    "\n",
    "# intialize the vector containing the variance of the estimator for each sample\n",
    "var_left = np.zeros_like(s)\n",
    "\n",
    "for i, h in enumerate(all_h):\n",
    "    g = all_g_left[i]\n",
    "\n",
    "    # compute the estimate for the scale h\n",
    "    yhat_h = convolve(s, g, mode=\"same\")\n",
    "\n",
    "    # compute the variance of the estimate\n",
    "    var_h = sigma**2 * convolve(np.ones_like(s), g**2, mode=\"same\")\n",
    "\n",
    "    # compute the lower and upper bound of the confidence interval for the scale h\n",
    "    lb = yhat_h - Gamma * np.sqrt(var_h)\n",
    "    ub = yhat_h + Gamma * np.sqrt(var_h)\n",
    "\n",
    "    # update the lower and upper bounds\n",
    "    lower_bounds = np.maximum(lower_bounds, lb)\n",
    "    upper_bounds = np.minimum(upper_bounds, ub)\n",
    "\n",
    "    # identify valid ICI points\n",
    "    valid_ici = lower_bounds <= upper_bounds\n",
    "\n",
    "    # update the estimate\n",
    "    yhat_left[valid_ici] = yhat_h[valid_ici]\n",
    "\n",
    "    # update the variance\n",
    "    var_left[valid_ici] = var_h[valid_ici]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iai8z6xLolaw"
   },
   "source": [
    "Use the LPA-ICI to compute the estimate based on the **right** kernels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "K7vfQJb9ola4"
   },
   "outputs": [],
   "source": [
    "yhat_right = np.zeros_like(s)\n",
    "# initialize the lower and upper bound vectors\n",
    "lower_bounds = -np.inf * np.ones(LENGTH)\n",
    "upper_bounds = np.inf * np.ones(LENGTH)\n",
    "\n",
    "# intialize the vector containing the variance of the estimator for each sample\n",
    "var_right = np.zeros_like(s)\n",
    "\n",
    "for i, h in enumerate(all_h):\n",
    "    g = all_g_right[i]  # Fixed: was using all_g_left[i]\n",
    "\n",
    "    # compute the estimate for the scale h\n",
    "    yhat_h = convolve(s, g, mode=\"same\")\n",
    "\n",
    "    # compute the variance of the estimate\n",
    "    var_h = sigma**2 * convolve(np.ones_like(s), g**2, mode=\"same\")\n",
    "\n",
    "    # compute the lower and upper bound of the confidence interval for the scale h\n",
    "    lb = yhat_h - Gamma * np.sqrt(var_h)\n",
    "    ub = yhat_h + Gamma * np.sqrt(var_h)\n",
    "\n",
    "    # update the lower and upper bounds\n",
    "    lower_bounds = np.maximum(lower_bounds, lb)\n",
    "    upper_bounds = np.minimum(upper_bounds, ub)\n",
    "\n",
    "    # identify valid ICI points\n",
    "    valid_ici = lower_bounds <= upper_bounds\n",
    "\n",
    "    # update the estimate\n",
    "    yhat_right[valid_ici] = yhat_h[valid_ici]\n",
    "\n",
    "    # update the variance\n",
    "    var_right[valid_ici] = var_h[valid_ici]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CSVwLUp8pFm4"
   },
   "source": [
    "Perform the aggregation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Jpw96MPppJL6"
   },
   "outputs": [],
   "source": [
    "weight_left = 1 / (var_left + 1e-10)  # add small epsilon to avoid division by zero\n",
    "weight_right = 1 / (var_right + 1e-10)\n",
    "total_weight = weight_left + weight_right\n",
    "\n",
    "yhat_aggr = (weight_left * yhat_left + weight_right * yhat_right) / total_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UlIPoQuvqH5v"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 10))\n",
    "plt.plot(ty, s, \"r.\")\n",
    "plt.plot(ty, y, \"k--\", linewidth=3)\n",
    "plt.plot(ty, yhat_right, \"m-\", linewidth=3)\n",
    "plt.plot(ty, yhat_left, \"g-\", linewidth=3)\n",
    "plt.plot(ty, yhat_aggr, \"b-\", linewidth=3)\n",
    "plt.grid()\n",
    "plt.legend(\n",
    "    [\"noisy\", \"original\", \"right estimate\", \"left estimate\", \"aggregated estimate\"]\n",
    ")\n",
    "plt.title(f\"N = {N:d}\")\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyNb3zFnnetv90zpaqEpgLju",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Image_processing_notes",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
