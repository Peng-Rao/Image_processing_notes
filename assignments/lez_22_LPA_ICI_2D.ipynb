{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "quAd5kUJXS2h"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from scipy.signal import convolve2d\n",
    "from skimage.io import imread"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4b4erfTUO7kA"
   },
   "outputs": [],
   "source": [
    "rootfolder = '.'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n9KEr7OUZ00l"
   },
   "source": [
    "Define the function to compute the kernel given the weights and the degree of the polynomial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ig2s_rwtZx6P"
   },
   "outputs": [],
   "source": [
    "def compute_2D_LPA_kernel(w, N):\n",
    "    # compute the 2D LPA kernel for a given weights and polynomial degree\n",
    "    # input:\n",
    "    #   w: matrix containing the weights for the local LS problem\n",
    "    #   N: degree of the polynomial approximation\n",
    "    # return:\n",
    "    #   g: the computed LPA kernel\n",
    "\n",
    "\n",
    "    # window size is the lenght of the weight vector\n",
    "    r, c = w.shape\n",
    "    M = r*c\n",
    "\n",
    "    # create the matrix T\n",
    "    tx = np.linspace(0, 1, c)\n",
    "    ty = np.linspace(0, 1, r)\n",
    "    tx, ty = np.meshgrid(tx, ty)\n",
    "    tx = tx.reshape(-1)\n",
    "    ty = ty.reshape(-1)\n",
    "    T = np.zeros((M,(N+1)**2))\n",
    "    cnt = 0\n",
    "    for i in range(N+1):\n",
    "        for j in range(N-i+1):\n",
    "            if i==0 and j==0:\n",
    "                T[:, cnt] = np.ones(M)\n",
    "            else:\n",
    "                T[:, cnt] = tx**i * ty**j\n",
    "            cnt = cnt + 1\n",
    "    T = T[:, :cnt]\n",
    "\n",
    "    # unroll the matrix of the weights\n",
    "#    w =\n",
    "\n",
    "    # generate the inverse of weights\n",
    "#    winv =\n",
    "\n",
    "    # set to zero weights that are inf\n",
    "#    winv\n",
    "\n",
    "    # define the weight matrix\n",
    "#    W =\n",
    "#    Winv =\n",
    "\n",
    "    ## construct the LPA kernel\n",
    "\n",
    "    # comput the qr decomposition of WT\n",
    "#    Q, R =\n",
    "\n",
    "    # define Qtilde\n",
    "#    Qtilde =\n",
    "\n",
    "    # adjust Qtilde with the weights matrix squared\n",
    "#    W2Qtilde =\n",
    "\n",
    "    # select the central row of W2Qtilde\n",
    "#    row =\n",
    "\n",
    "    # compute the kernel\n",
    "#    g_bar =\n",
    "\n",
    "    #reshape the kernel in a matrix\n",
    "#    g_bar =\n",
    "\n",
    "    # flipping, since it is used in convolution\n",
    "#    g =\n",
    "\n",
    "    return g\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gcNKF8y9O46G"
   },
   "source": [
    "Load the image and add the noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pp5KWkxlO6lQ"
   },
   "outputs": [],
   "source": [
    "img = imread(f'{rootfolder}/data/cameraman.png') / 255\n",
    "\n",
    "sigma_noise = 20/255\n",
    "noisy_img = img + np.random.normal(size=img.shape) * sigma_noise\n",
    "\n",
    "#psnr_noisy =\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tsxElZddPSs9"
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 2, figsize=(20, 10))\n",
    "ax[0].imshow(img, cmap='gray')\n",
    "ax[0].set_title('Original image')\n",
    "\n",
    "ax[1].imshow(noisy_img, cmap='gray')\n",
    "ax[1].set_title(f'Noisy image, PSNR = {psnr_noisy:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "34S3SYMckqtD"
   },
   "source": [
    "LPA-ICI 2D\n",
    "----------\n",
    "Set the LPA-ICI parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yYPg7PvuaT9Y"
   },
   "outputs": [],
   "source": [
    "# maximum degree of polynomial used for fitting\n",
    "N = 1\n",
    "\n",
    "# parameter for the confidence intervals in the ICI rule\n",
    "Gamma = 2\n",
    "\n",
    "# Set all the scale values\n",
    "hmax = 21\n",
    "all_h = np.arange(1, hmax+1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DMZBeqQtcunJ"
   },
   "source": [
    "Generate the LPA kernels for all the scales. Use centered weights.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qUUogpW_qmdi"
   },
   "outputs": [],
   "source": [
    "all_g = []\n",
    "for i, h in enumerate(all_h):\n",
    "        # define the weights for the scale h symmetric\n",
    "\n",
    "        # size of the weight MATRIX\n",
    "#        w =\n",
    "\n",
    "        # compute and store the kernel g\n",
    "        g = compute_2D_LPA_kernel(w, N)\n",
    "        all_g.append(g)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fBP6KA27s_kF"
   },
   "source": [
    "Initialize all the variables for the ICI rule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "23d65QPcrRso"
   },
   "outputs": [],
   "source": [
    "# initialize the estimate for each scale\n",
    "yhat = np.zeros((img.shape))\n",
    "\n",
    "# initialize the vector containing the best scale for each sample\n",
    "best_scale = np.zeros(shape=yhat.shape)\n",
    "\n",
    "# initialize the lower and upper bound matrices\n",
    "lower_bounds = - np.inf * np.ones(shape=yhat.shape)\n",
    "upper_bounds = np.inf * np.ones(shape=yhat.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AaCGFuCrlbGI"
   },
   "source": [
    "Loop over all the scales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5nuG5GihlgQ3"
   },
   "outputs": [],
   "source": [
    "for i, h in enumerate(all_h):\n",
    "    g = all_g[i]\n",
    "\n",
    "    # compute the estimate for the scale h\n",
    "#    yhat_h =\n",
    "\n",
    "    # compute the lower and upper bound of the confidence interval for the scale h\n",
    "#    lb =\n",
    "#    ub =\n",
    "\n",
    "    # update the lower and upper bounds\n",
    "#    lower_bounds =\n",
    "#    upper_bounds =\n",
    "\n",
    "    # identify for which samples h is the best scale according to the\n",
    "    # ICI rule and update the best_scale vector accordingly\n",
    "    # update best_scale\n",
    "\n",
    "    # update the estimate\n",
    "    # update yhat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dVx-E5q8tCs3"
   },
   "source": [
    "Compute the PSNR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GHi9A8uXqryE"
   },
   "outputs": [],
   "source": [
    "# psnr ="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tBJUSUuqdp7v"
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 2, figsize=(20, 10))\n",
    "ax[0].imshow(yhat, cmap='gray')\n",
    "ax[0].set_title(f'LPA-ICI estimate, PSNR = {psnr:.2f}')\n",
    "\n",
    "ax[1].imshow(best_scale)\n",
    "ax[1].set_title('Best scale for each pixel')\n",
    "fig.colorbar(ax[1].pcolormesh(best_scale), ax=ax[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kMvwf8ram7Lf"
   },
   "source": [
    "Anisotropic LPA-ICI\n",
    "------------------------\n",
    "Set the parameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZtsjdjSWnENQ"
   },
   "outputs": [],
   "source": [
    "# maximum degree of polynomial used for fitting\n",
    "N = 1\n",
    "\n",
    "# parameter for the confidence intervals in the ICI rule\n",
    "Gamma = 2\n",
    "\n",
    "# Set all the scale values\n",
    "hmax = 21\n",
    "all_h = np.arange(1, hmax+1)\n",
    "\n",
    "# set all the direction values\n",
    "all_theta = np.arange(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tfg3XYvInP9p"
   },
   "source": [
    "Generate the LPA kernels for all the scales and all the directions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oy5unf_onSWi"
   },
   "outputs": [],
   "source": [
    "all_g = []\n",
    "\n",
    "for theta in all_theta:\n",
    "\n",
    "    all_g_theta = []\n",
    "    for i, h in enumerate(all_h):\n",
    "        # define the weights for the scale h and the direction theta\n",
    "#        if theta\n",
    "#            w =\n",
    "\n",
    "        # compute and store the kernel g\n",
    "        g = compute_2D_LPA_kernel(w, N)\n",
    "\n",
    "        all_g_theta.append(g)\n",
    "\n",
    "    all_g.append(all_g_theta)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OPapyBO_oFEj"
   },
   "source": [
    "Initialize all the variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pNrjeHXOoRbo"
   },
   "outputs": [],
   "source": [
    "# initialize the estimate for each scale\n",
    "yhat = np.zeros(img.shape)\n",
    "\n",
    "# initialize the matrix of the aggregation weights\n",
    "weights = np.zeros(img.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r46En2rOWKtF"
   },
   "source": [
    "Use the LPA-ICI to compute find the best scale for each direction and compute the finale estimates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qdOODckDWJVn"
   },
   "outputs": [],
   "source": [
    "# loop over all the directions\n",
    "for theta in all_theta:\n",
    "    # initialize the estimate for the direction theta\n",
    "    yhat_theta = np.zeros(img.shape)\n",
    "\n",
    "    # initialize the matrix all the variances for the direction theta\n",
    "    var_theta = np.zeros(img.shape)\n",
    "\n",
    "    # initialize the lower and upper bounds matrices\n",
    "    # lower_bounds =\n",
    "    # upper_bounds =\n",
    "\n",
    "    # loop over all scales\n",
    "    all_g_theta = all_g[theta]\n",
    "    for i, h in enumerate(all_h):\n",
    "        g = all_g_theta[i]\n",
    "\n",
    "        # compute the estimate for the scale h\n",
    "#        yhat_h =\n",
    "\n",
    "        # compute the lower and upper bound of the confidence interval for the scale h\n",
    "#        lb =\n",
    "#        ub =\n",
    "\n",
    "        # update the lower and upper bounds\n",
    "#        lower_bounds =\n",
    "#        upper_bounds =\n",
    "\n",
    "        # update the estimate\n",
    "        # update yhat_theta\n",
    "\n",
    "        # update the matrix with the variances\n",
    "        # update var_theta\n",
    "\n",
    "    # update the estimates and the weights\n",
    "    # yhat =\n",
    "    # weights =\n",
    "\n",
    "# compute the final estimates\n",
    "yhat = yhat / weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_6fDHh9FYHk9"
   },
   "source": [
    "Compute the PSNR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xvJft9nYYJQo"
   },
   "outputs": [],
   "source": [
    "# psnr ="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bioSFH9dYHPB"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,10))\n",
    "plt.imshow(yhat, cmap='gray')\n",
    "plt.title(f'LPA-ICI estimate, PSNR = {psnr:.2f}')\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
