{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "quAd5kUJXS2h"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from scipy.signal import convolve2d\n",
    "from skimage.io import imread\n",
    "\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format='retina'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4b4erfTUO7kA"
   },
   "outputs": [],
   "source": [
    "rootfolder = \"..\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n9KEr7OUZ00l"
   },
   "source": [
    "Define the function to compute the kernel given the weights and the degree of the polynomial\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ig2s_rwtZx6P"
   },
   "outputs": [],
   "source": [
    "def compute_2D_LPA_kernel(w, N):\n",
    "    \"\"\"\n",
    "    Compute the 2D LPA kernel for a given weights and polynomial degree\n",
    "\n",
    "    Input:\n",
    "        w: matrix containing the weights for the local LS problem\n",
    "        N: degree of the polynomial approximation\n",
    "    Return:\n",
    "        g: the computed LPA kernel\n",
    "    \"\"\"\n",
    "    # window size is the lenght of the weight vector\n",
    "    r, c = w.shape\n",
    "    M = r * c\n",
    "\n",
    "    # create the matrix T\n",
    "    tx = np.linspace(0, 1, c)\n",
    "    ty = np.linspace(0, 1, r)\n",
    "    tx, ty = np.meshgrid(tx, ty)\n",
    "    tx = tx.reshape(-1)\n",
    "    ty = ty.reshape(-1)\n",
    "    T = np.zeros((M, (N + 1) ** 2))\n",
    "    cnt = 0\n",
    "    for i in range(N + 1):\n",
    "        for j in range(N - i + 1):\n",
    "            if i == 0 and j == 0:\n",
    "                T[:, cnt] = np.ones(M)\n",
    "            else:\n",
    "                T[:, cnt] = tx**i * ty**j\n",
    "            cnt = cnt + 1\n",
    "    T = T[:, :cnt]\n",
    "\n",
    "    # unroll the matrix of the weights\n",
    "    w = w.reshape(-1)\n",
    "\n",
    "    # generate the inverse of weights\n",
    "    winv = np.zeros_like(w)\n",
    "    winv[w != 0] = 1 / w[w != 0]\n",
    "\n",
    "    # set to zero weights that are inf\n",
    "    winv[np.isinf(winv)] = 0\n",
    "\n",
    "    # define the weight matrix\n",
    "    W = np.diag(w)\n",
    "    Winv = np.diag(winv)\n",
    "\n",
    "    ## construct the LPA kernel\n",
    "\n",
    "    # compute the qr decomposition of WT\n",
    "    Q, R = np.linalg.qr(W @ T)\n",
    "\n",
    "    # define Qtilde\n",
    "    Qtilde = Winv @ Q\n",
    "\n",
    "    # adjust Qtilde with the weights matrix squared\n",
    "    W2Qtilde = W @ W @ Qtilde\n",
    "\n",
    "    # select the central row of W2Qtilde\n",
    "    row = M // 2\n",
    "\n",
    "    # compute the kernel\n",
    "    g_bar = W2Qtilde[row, 0] * Qtilde[:, 0]\n",
    "\n",
    "    # reshape the kernel in a matrix\n",
    "    g_bar = g_bar.reshape(r, c)\n",
    "\n",
    "    # flipping, since it is used in convolution\n",
    "    g = np.flip(g_bar)\n",
    "\n",
    "    return g"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gcNKF8y9O46G"
   },
   "source": [
    "Load the image and add the noise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pp5KWkxlO6lQ"
   },
   "outputs": [],
   "source": [
    "img = imread(f\"{rootfolder}/data/cameraman.png\") / 255\n",
    "\n",
    "sigma_noise = 20 / 255\n",
    "noisy_img = img + np.random.normal(size=img.shape) * sigma_noise\n",
    "\n",
    "psnr_noisy = 10 * np.log10(1 / np.mean((noisy_img - img) ** 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tsxElZddPSs9"
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 2, figsize=(20, 10))\n",
    "ax[0].imshow(img, cmap=\"gray\")\n",
    "ax[0].set_title(\"Original image\")\n",
    "\n",
    "ax[1].imshow(noisy_img, cmap=\"gray\")\n",
    "ax[1].set_title(f\"Noisy image, PSNR = {psnr_noisy:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "34S3SYMckqtD"
   },
   "source": [
    "## LPA-ICI 2D\n",
    "\n",
    "Set the LPA-ICI parameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yYPg7PvuaT9Y"
   },
   "outputs": [],
   "source": [
    "# maximum degree of polynomial used for fitting\n",
    "N = 1\n",
    "\n",
    "# parameter for the confidence intervals in the ICI rule\n",
    "Gamma = 2\n",
    "\n",
    "# Set all the scale values\n",
    "hmax = 21\n",
    "all_h = np.arange(1, hmax + 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DMZBeqQtcunJ"
   },
   "source": [
    "Generate the LPA kernels for all the scales. Use centered weights.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qUUogpW_qmdi"
   },
   "outputs": [],
   "source": [
    "all_g = []\n",
    "for i, h in enumerate(all_h):\n",
    "    # define the weights for the scale h symmetric\n",
    "    # size of the weight MATRIX\n",
    "    w = np.zeros((2 * hmax + 1, 2 * hmax + 1))\n",
    "    w[hmax - h : hmax + h + 1, hmax - h : hmax + h + 1] = 1\n",
    "    # compute and store the kernel g\n",
    "    g = compute_2D_LPA_kernel(w, N)\n",
    "    all_g.append(g)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fBP6KA27s_kF"
   },
   "source": [
    "Initialize all the variables for the ICI rule\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "23d65QPcrRso"
   },
   "outputs": [],
   "source": [
    "# initialize the estimate for each scale\n",
    "yhat = np.zeros((img.shape))\n",
    "\n",
    "# initialize the vector containing the best scale for each sample\n",
    "best_scale = np.zeros(shape=yhat.shape)\n",
    "\n",
    "# initialize the lower and upper bound matrices\n",
    "lower_bounds = -np.inf * np.ones(shape=yhat.shape)\n",
    "upper_bounds = np.inf * np.ones(shape=yhat.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AaCGFuCrlbGI"
   },
   "source": [
    "Loop over all the scales\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5nuG5GihlgQ3"
   },
   "outputs": [],
   "source": [
    "# Loop over all the scales\n",
    "for i, h in enumerate(all_h):\n",
    "    g = all_g[i]\n",
    "\n",
    "    # compute the estimate for the scale h\n",
    "    yhat_h = convolve2d(noisy_img, g, mode=\"same\", boundary=\"symm\")\n",
    "\n",
    "    # compute the variance\n",
    "    var_h = sigma_noise**2 * convolve2d(\n",
    "        np.ones_like(noisy_img), g**2, mode=\"same\", boundary=\"symm\"\n",
    "    )\n",
    "\n",
    "    # compute the lower and upper bound of the confidence interval for the scale h\n",
    "    lb = yhat_h - Gamma * np.sqrt(var_h)\n",
    "    ub = yhat_h + Gamma * np.sqrt(var_h)\n",
    "\n",
    "    # update the lower and upper bounds\n",
    "    lower_bounds = np.maximum(lower_bounds, lb)\n",
    "    upper_bounds = np.minimum(upper_bounds, ub)\n",
    "\n",
    "    # identify for which samples h is the best scale according to the\n",
    "    # ICI rule and update the best_scale vector accordingly\n",
    "    valid_ici = lower_bounds <= upper_bounds\n",
    "    best_scale[valid_ici] = h\n",
    "\n",
    "    # update the estimate\n",
    "    yhat[valid_ici] = yhat_h[valid_ici]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dVx-E5q8tCs3"
   },
   "source": [
    "Compute the PSNR\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GHi9A8uXqryE"
   },
   "outputs": [],
   "source": [
    "psnr = 10 * np.log10(1 / np.mean((yhat - img) ** 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tBJUSUuqdp7v"
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 2, figsize=(20, 10))\n",
    "ax[0].imshow(yhat, cmap=\"gray\")\n",
    "ax[0].set_title(f\"LPA-ICI estimate, PSNR = {psnr:.2f}\")\n",
    "\n",
    "ax[1].imshow(best_scale)\n",
    "ax[1].set_title(\"Best scale for each pixel\")\n",
    "fig.colorbar(ax[1].pcolormesh(best_scale), ax=ax[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kMvwf8ram7Lf"
   },
   "source": [
    "## Anisotropic LPA-ICI\n",
    "\n",
    "Set the parameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZtsjdjSWnENQ"
   },
   "outputs": [],
   "source": [
    "# maximum degree of polynomial used for fitting\n",
    "N = 1\n",
    "\n",
    "# parameter for the confidence intervals in the ICI rule\n",
    "Gamma = 2\n",
    "\n",
    "# Set all the scale values\n",
    "hmax = 21\n",
    "all_h = np.arange(1, hmax + 1)\n",
    "\n",
    "# set all the direction values\n",
    "all_theta = np.arange(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tfg3XYvInP9p"
   },
   "source": [
    "Generate the LPA kernels for all the scales and all the directions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oy5unf_onSWi"
   },
   "outputs": [],
   "source": [
    "all_g = []\n",
    "\n",
    "for theta in all_theta:\n",
    "    all_g_theta = []\n",
    "    for i, h in enumerate(all_h):\n",
    "        # define the weights for the scale h and the direction theta\n",
    "        w = np.zeros((2 * hmax + 1, 2 * hmax + 1))\n",
    "\n",
    "        if theta == 0:  # Horizontal\n",
    "            w[hmax, hmax - h : hmax + h + 1] = 1\n",
    "        elif theta == 1:  # Vertical\n",
    "            w[hmax - h : hmax + h + 1, hmax] = 1\n",
    "        elif theta == 2:  # Diagonal 1 (top-left to bottom-right)\n",
    "            for k in range(-h, h + 1):\n",
    "                if 0 <= hmax + k < 2 * hmax + 1:\n",
    "                    w[hmax + k, hmax + k] = 1\n",
    "        elif theta == 3:  # Diagonal 2 (top-right to bottom-left)\n",
    "            for k in range(-h, h + 1):\n",
    "                if 0 <= hmax + k < 2 * hmax + 1 and 0 <= hmax - k < 2 * hmax + 1:\n",
    "                    w[hmax + k, hmax - k] = 1\n",
    "\n",
    "        # compute and store the kernel g\n",
    "        g = compute_2D_LPA_kernel(w, N)\n",
    "        all_g_theta.append(g)\n",
    "\n",
    "    all_g.append(all_g_theta)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OPapyBO_oFEj"
   },
   "source": [
    "Initialize all the variables\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pNrjeHXOoRbo"
   },
   "outputs": [],
   "source": [
    "# initialize the estimate for each scale\n",
    "yhat = np.zeros(img.shape)\n",
    "\n",
    "# initialize the matrix of the aggregation weights\n",
    "weights = np.zeros(img.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r46En2rOWKtF"
   },
   "source": [
    "Use the LPA-ICI to compute find the best scale for each direction and compute the finale estimates\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qdOODckDWJVn"
   },
   "outputs": [],
   "source": [
    "# loop over all the directions\n",
    "for theta in all_theta:\n",
    "    # initialize the estimate for the direction theta\n",
    "    yhat_theta = np.zeros(img.shape)\n",
    "\n",
    "    # initialize the matrix all the variances for the direction theta\n",
    "    var_theta = np.zeros(img.shape)\n",
    "\n",
    "    # initialize the lower and upper bounds matrices\n",
    "    lower_bounds = -np.inf * np.ones(img.shape)\n",
    "    upper_bounds = np.inf * np.ones(img.shape)\n",
    "\n",
    "    # loop over all scales\n",
    "    all_g_theta = all_g[theta]\n",
    "    for i, h in enumerate(all_h):\n",
    "        g = all_g_theta[i]\n",
    "\n",
    "        # compute the estimate for the scale h\n",
    "        yhat_h = convolve2d(noisy_img, g, mode=\"same\", boundary=\"symm\")\n",
    "\n",
    "        # compute the variance\n",
    "        var_h = sigma_noise**2 * convolve2d(\n",
    "            np.ones_like(noisy_img), g**2, mode=\"same\", boundary=\"symm\"\n",
    "        )\n",
    "\n",
    "        # compute the lower and upper bound of the confidence interval for the scale h\n",
    "        lb = yhat_h - Gamma * np.sqrt(var_h)\n",
    "        ub = yhat_h + Gamma * np.sqrt(var_h)\n",
    "\n",
    "        # update the lower and upper bounds\n",
    "        lower_bounds = np.maximum(lower_bounds, lb)\n",
    "        upper_bounds = np.minimum(upper_bounds, ub)\n",
    "\n",
    "        # identify valid ICI points\n",
    "        valid_ici = lower_bounds <= upper_bounds\n",
    "\n",
    "        # update the estimate\n",
    "        yhat_theta[valid_ici] = yhat_h[valid_ici]\n",
    "\n",
    "        # update the matrix with the variances\n",
    "        var_theta[valid_ici] = var_h[valid_ici]\n",
    "\n",
    "    # update the estimates and the weights (inverse variance weighting)\n",
    "    weight_theta = 1 / (var_theta + 1e-10)\n",
    "    yhat = yhat + weight_theta * yhat_theta\n",
    "    weights = weights + weight_theta\n",
    "\n",
    "yhat = yhat / weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_6fDHh9FYHk9"
   },
   "source": [
    "Compute the PSNR\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xvJft9nYYJQo"
   },
   "outputs": [],
   "source": [
    "psnr = 10 * np.log10(1 / np.mean((yhat - img) ** 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bioSFH9dYHPB"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 10))\n",
    "plt.imshow(yhat, cmap=\"gray\")\n",
    "plt.title(f\"LPA-ICI estimate, PSNR = {psnr:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Summary comparison figure\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 2, figsize=(15, 15))\n",
    "\n",
    "axes[0, 0].imshow(img, cmap=\"gray\", vmin=0, vmax=1)\n",
    "axes[0, 0].set_title(\"Original Image\")\n",
    "axes[0, 0].axis(\"off\")\n",
    "\n",
    "axes[0, 1].imshow(noisy_img, cmap=\"gray\", vmin=0, vmax=1)\n",
    "axes[0, 1].set_title(f\"Noisy Image (PSNR = {psnr_noisy:.2f} dB)\")\n",
    "axes[0, 1].axis(\"off\")\n",
    "\n",
    "# Re-run isotropic LPA-ICI for comparison\n",
    "yhat_iso = np.zeros((img.shape))\n",
    "best_scale_iso = np.zeros(shape=yhat_iso.shape)\n",
    "lower_bounds = -np.inf * np.ones(shape=yhat_iso.shape)\n",
    "upper_bounds = np.inf * np.ones(shape=yhat_iso.shape)\n",
    "\n",
    "for i, h in enumerate(all_h):\n",
    "    w = np.zeros((2 * hmax + 1, 2 * hmax + 1))\n",
    "    w[hmax - h : hmax + h + 1, hmax - h : hmax + h + 1] = 1\n",
    "    g = compute_2D_LPA_kernel(w, N)\n",
    "\n",
    "    yhat_h = convolve2d(noisy_img, g, mode=\"same\", boundary=\"symm\")\n",
    "    var_h = sigma_noise**2 * convolve2d(\n",
    "        np.ones_like(noisy_img), g**2, mode=\"same\", boundary=\"symm\"\n",
    "    )\n",
    "\n",
    "    lb = yhat_h - Gamma * np.sqrt(var_h)\n",
    "    ub = yhat_h + Gamma * np.sqrt(var_h)\n",
    "\n",
    "    lower_bounds = np.maximum(lower_bounds, lb)\n",
    "    upper_bounds = np.minimum(upper_bounds, ub)\n",
    "\n",
    "    valid_ici = lower_bounds <= upper_bounds\n",
    "    best_scale_iso[valid_ici] = h\n",
    "    yhat_iso[valid_ici] = yhat_h[valid_ici]\n",
    "\n",
    "psnr_iso = 10 * np.log10(1 / np.mean((yhat_iso - img) ** 2))\n",
    "\n",
    "axes[1, 0].imshow(yhat_iso, cmap=\"gray\", vmin=0, vmax=1)\n",
    "axes[1, 0].set_title(f\"Isotropic LPA-ICI (PSNR = {psnr_iso:.2f} dB)\")\n",
    "axes[1, 0].axis(\"off\")\n",
    "\n",
    "axes[1, 1].imshow(yhat, cmap=\"gray\", vmin=0, vmax=1)\n",
    "axes[1, 1].set_title(f\"Anisotropic LPA-ICI (PSNR = {psnr:.2f} dB)\")\n",
    "axes[1, 1].axis(\"off\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"PSNR Results:\")\n",
    "print(f\"Noisy image: {psnr_noisy:.2f} dB\")\n",
    "print(f\"Isotropic LPA-ICI: {psnr_iso:.2f} dB\")\n",
    "print(f\"Anisotropic LPA-ICI: {psnr:.2f} dB\")\n",
    "print(f\"Improvement (Anisotropic vs Isotropic): {psnr - psnr_iso:.2f} dB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement the LPA-ICI with directional kernels (defined over the quadrants)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## LPA-ICI with Quadrant-based Directional Kernels\n",
    "\n",
    "# Set the parameters\n",
    "N = 1  # maximum degree of polynomial used for fitting\n",
    "Gamma = 2  # parameter for the confidence intervals in the ICI rule\n",
    "hmax = 21\n",
    "all_h = np.arange(1, hmax + 1)\n",
    "\n",
    "# Define 4 quadrant directions\n",
    "# 0: Top-right quadrant\n",
    "# 1: Top-left quadrant\n",
    "# 2: Bottom-left quadrant\n",
    "# 3: Bottom-right quadrant\n",
    "all_quadrants = np.arange(4)\n",
    "\n",
    "# Generate the LPA kernels for all scales and all quadrant directions\n",
    "all_g_quad = []\n",
    "\n",
    "for quad in all_quadrants:\n",
    "    all_g_quad_direction = []\n",
    "    for i, h in enumerate(all_h):\n",
    "        # Define the weights for the scale h and the quadrant direction\n",
    "        w = np.zeros((2 * hmax + 1, 2 * hmax + 1))\n",
    "\n",
    "        if quad == 0:  # Top-right quadrant\n",
    "            for dy in range(-h, 1):  # y from -h to 0\n",
    "                for dx in range(0, h + 1):  # x from 0 to h\n",
    "                    if hmax + dy >= 0 and hmax + dx < 2 * hmax + 1:\n",
    "                        w[hmax + dy, hmax + dx] = 1\n",
    "\n",
    "        elif quad == 1:  # Top-left quadrant\n",
    "            for dy in range(-h, 1):  # y from -h to 0\n",
    "                for dx in range(-h, 1):  # x from -h to 0\n",
    "                    if hmax + dy >= 0 and hmax + dx >= 0:\n",
    "                        w[hmax + dy, hmax + dx] = 1\n",
    "\n",
    "        elif quad == 2:  # Bottom-left quadrant\n",
    "            for dy in range(0, h + 1):  # y from 0 to h\n",
    "                for dx in range(-h, 1):  # x from -h to 0\n",
    "                    if hmax + dy < 2 * hmax + 1 and hmax + dx >= 0:\n",
    "                        w[hmax + dy, hmax + dx] = 1\n",
    "\n",
    "        elif quad == 3:  # Bottom-right quadrant\n",
    "            for dy in range(0, h + 1):  # y from 0 to h\n",
    "                for dx in range(0, h + 1):  # x from 0 to h\n",
    "                    if hmax + dy < 2 * hmax + 1 and hmax + dx < 2 * hmax + 1:\n",
    "                        w[hmax + dy, hmax + dx] = 1\n",
    "\n",
    "        # Compute and store the kernel g\n",
    "        g = compute_2D_LPA_kernel(w, N)\n",
    "        all_g_quad_direction.append(g)\n",
    "\n",
    "    all_g_quad.append(all_g_quad_direction)\n",
    "\n",
    "# Initialize variables for quadrant-based LPA-ICI\n",
    "yhat_quad = np.zeros(img.shape)\n",
    "weights_quad = np.zeros(img.shape)\n",
    "\n",
    "# Loop over all quadrant directions\n",
    "for quad in all_quadrants:\n",
    "    # Initialize the estimate for the current quadrant direction\n",
    "    yhat_quad_direction = np.zeros(img.shape)\n",
    "\n",
    "    # Initialize the matrix of variances for the current quadrant direction\n",
    "    var_quad_direction = np.zeros(img.shape)\n",
    "\n",
    "    # Initialize the lower and upper bounds matrices\n",
    "    lower_bounds = -np.inf * np.ones(img.shape)\n",
    "    upper_bounds = np.inf * np.ones(img.shape)\n",
    "\n",
    "    # Loop over all scales for current quadrant\n",
    "    all_g_quad_direction = all_g_quad[quad]\n",
    "    for i, h in enumerate(all_h):\n",
    "        g = all_g_quad_direction[i]\n",
    "\n",
    "        # Compute the estimate for the scale h\n",
    "        yhat_h = convolve2d(noisy_img, g, mode=\"same\", boundary=\"symm\")\n",
    "\n",
    "        # Compute the variance\n",
    "        var_h = sigma_noise**2 * convolve2d(\n",
    "            np.ones_like(noisy_img), g**2, mode=\"same\", boundary=\"symm\"\n",
    "        )\n",
    "\n",
    "        # Compute the lower and upper bound of the confidence interval for the scale h\n",
    "        lb = yhat_h - Gamma * np.sqrt(var_h)\n",
    "        ub = yhat_h + Gamma * np.sqrt(var_h)\n",
    "\n",
    "        # Update the lower and upper bounds\n",
    "        lower_bounds = np.maximum(lower_bounds, lb)\n",
    "        upper_bounds = np.minimum(upper_bounds, ub)\n",
    "\n",
    "        # Identify valid ICI points\n",
    "        valid_ici = lower_bounds <= upper_bounds\n",
    "\n",
    "        # Update the estimate\n",
    "        yhat_quad_direction[valid_ici] = yhat_h[valid_ici]\n",
    "\n",
    "        # Update the matrix with the variances\n",
    "        var_quad_direction[valid_ici] = var_h[valid_ici]\n",
    "\n",
    "    # Update the estimates and the weights (inverse variance weighting)\n",
    "    weight_quad_direction = 1 / (var_quad_direction + 1e-10)\n",
    "    yhat_quad = yhat_quad + weight_quad_direction * yhat_quad_direction\n",
    "    weights_quad = weights_quad + weight_quad_direction\n",
    "\n",
    "# Final estimate\n",
    "yhat_quad = yhat_quad / weights_quad\n",
    "\n",
    "# Compute the PSNR for quadrant-based method\n",
    "psnr_quad = 10 * np.log10(1 / np.mean((yhat_quad - img) ** 2))\n",
    "\n",
    "print(f\"Quadrant-based LPA-ICI PSNR: {psnr_quad:.2f} dB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization of results comparison\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "\n",
    "# Original image\n",
    "axes[0, 0].imshow(img, cmap=\"gray\", vmin=0, vmax=1)\n",
    "axes[0, 0].set_title(\"Original Image\")\n",
    "axes[0, 0].axis(\"off\")\n",
    "\n",
    "# Noisy image\n",
    "axes[0, 1].imshow(noisy_img, cmap=\"gray\", vmin=0, vmax=1)\n",
    "axes[0, 1].set_title(f\"Noisy Image\\n(PSNR = {psnr_noisy:.2f} dB)\")\n",
    "axes[0, 1].axis(\"off\")\n",
    "\n",
    "# Isotropic LPA-ICI\n",
    "axes[0, 2].imshow(yhat_iso, cmap=\"gray\", vmin=0, vmax=1)\n",
    "axes[0, 2].set_title(f\"Isotropic LPA-ICI\\n(PSNR = {psnr_iso:.2f} dB)\")\n",
    "axes[0, 2].axis(\"off\")\n",
    "\n",
    "# Anisotropic LPA-ICI (line-based)\n",
    "axes[1, 0].imshow(yhat, cmap=\"gray\", vmin=0, vmax=1)\n",
    "axes[1, 0].set_title(f\"Line-based Anisotropic\\n(PSNR = {psnr:.2f} dB)\")\n",
    "axes[1, 0].axis(\"off\")\n",
    "\n",
    "# Quadrant-based LPA-ICI\n",
    "axes[1, 1].imshow(yhat_quad, cmap=\"gray\", vmin=0, vmax=1)\n",
    "axes[1, 1].set_title(f\"Quadrant-based Anisotropic\\n(PSNR = {psnr_quad:.2f} dB)\")\n",
    "axes[1, 1].axis(\"off\")\n",
    "\n",
    "# Difference image (quadrant vs line-based)\n",
    "diff_img = np.abs(yhat_quad - yhat)\n",
    "im = axes[1, 2].imshow(diff_img, cmap=\"hot\")\n",
    "axes[1, 2].set_title(\"Absolute Difference\\n(Quadrant vs Line-based)\")\n",
    "axes[1, 2].axis(\"off\")\n",
    "plt.colorbar(im, ax=axes[1, 2], shrink=0.6)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print summary of results\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"PERFORMANCE COMPARISON\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Noisy image:                {psnr_noisy:.2f} dB\")\n",
    "print(f\"Isotropic LPA-ICI:          {psnr_iso:.2f} dB\")\n",
    "print(f\"Line-based Anisotropic:     {psnr:.2f} dB\")\n",
    "print(f\"Quadrant-based Anisotropic: {psnr_quad:.2f} dB\")\n",
    "print(\"-\" * 50)\n",
    "print(\"Improvement over isotropic:\")\n",
    "print(f\"  Line-based:     {psnr - psnr_iso:.2f} dB\")\n",
    "print(f\"  Quadrant-based: {psnr_quad - psnr_iso:.2f} dB\")\n",
    "print(f\"Quadrant vs Line difference: {psnr_quad - psnr:.2f} dB\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Image_processing_notes",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
