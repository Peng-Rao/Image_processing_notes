\documentclass[12pt]{article}
\usepackage[margin=1in]{geometry}
\usepackage{amsmath, amsthm, amssymb, mathtools, mathrsfs}
\usepackage{enumitem, graphicx, hyperref, booktabs, array}
\usepackage[mathscr]{euscript}
\usepackage[T1]{fontenc}
\usepackage{microtype}
\hypersetup{colorlinks=true, linkcolor=blue, citecolor=red}

% Custom commands for notation consistency
\renewcommand{\vec}[1]{\mathbf{#1}}
\DeclareMathOperator{\prox}{prox}
\DeclareMathOperator{\argmin}{arg\,min}
\DeclareMathOperator{\argmax}{arg\,max}
\DeclareMathOperator{\sign}{sign}
\DeclareMathOperator{\soft}{soft}
\newcommand{\norm}[1]{\left\lVert#1\right\rVert}
\newcommand{\abs}[1]{\left|#1\right|}
\newcommand{\inner}[2]{\langle #1, #2 \rangle}
\newcommand{\R}{\mathbb{R}}

% Theorem environments
\theoremstyle{definition}
\newtheorem{definition}{Definition}[section]
\newtheorem{theorem}{Theorem}[section]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{remark}{Remark}[section]
\newtheorem{example}{Example}[section]

\title{Proximal Gradient Methods and Iterative Soft Thresholding:\\
A Comprehensive Analysis for Sparse Optimization}
\author{Lecture Notes}
\date{\today}

\begin{document}
\maketitle

\tableofcontents
\newpage

%==============================================================================
\section{Introduction and Motivation}
%==============================================================================

\subsection{The Sparse Recovery Problem}

The fundamental challenge in modern signal processing and machine learning involves recovering sparse representations from noisy observations. Consider the linear model:
\begin{equation}
    \vec{y} = \vec{D}\vec{x} + \vec{\epsilon}
    \label{eq:linear_model}
\end{equation}
where $\vec{y} \in \R^m$ represents observed measurements, $\vec{D} \in \R^{m \times n}$ is a dictionary matrix (often overcomplete with $n > m$), $\vec{x} \in \R^n$ is the sparse coefficient vector we seek to recover, and $\vec{\epsilon} \in \R^m$ denotes additive noise.

The sparsity constraint fundamentally transforms this inverse problem. Rather than seeking any solution satisfying the data fidelity constraint, we specifically desire solutions where most components of $\vec{x}$ equal zero. This preference leads to the basis pursuit denoising (BPDN) formulation:

\begin{equation}
    \min_{\vec{x} \in \R^n} \frac{1}{2}\norm{\vec{D}\vec{x} - \vec{y}}_2^2 + \lambda\norm{\vec{x}}_1
    \label{eq:bpdn}
\end{equation}

\subsection{Mathematical Challenges}

The optimization problem \eqref{eq:bpdn} presents a fundamental mathematical challenge: while the data fidelity term $\frac{1}{2}\norm{\vec{D}\vec{x} - \vec{y}}_2^2$ is smooth and convex with Lipschitz continuous gradient, the $\ell_1$ regularization term $\lambda\norm{\vec{x}}_1$ is non-differentiable at the origin. Specifically:

\begin{align}
    \nabla_{\vec{x}} \left[\frac{1}{2}\norm{\vec{D}\vec{x} - \vec{y}}_2^2\right] & = \vec{D}^T(\vec{D}\vec{x} - \vec{y}) \quad \text{(well-defined everywhere)} \\
    \partial \norm{\vec{x}}_1                                                    & = \begin{cases}
                                                                                         \{\sign(\vec{x})\} & \text{if } \vec{x} \neq \vec{0} \\
                                                                                         [-1,1]^n           & \text{if } \vec{x} = \vec{0}
                                                                                     \end{cases}
\end{align}

% NOTE: The subdifferential notation emphasizes the non-smooth nature of the l1 norm

This non-differentiability precludes direct application of gradient descent methods, necessitating more sophisticated optimization techniques.

\newpage
%==============================================================================
\section{Mathematical Foundations: Convex Analysis and Majorization}
%==============================================================================

\subsection{Convexity and Smoothness}

\begin{definition}[Convex Function]
    A function $f: \R^n \to \R \cup \{+\infty\}$ is \textit{convex} if for all $\vec{x}, \vec{y} \in \operatorname{dom}(f)$ and $\theta \in [0,1]$:
    \begin{equation}
        f(\theta\vec{x} + (1-\theta)\vec{y}) \leq \theta f(\vec{x}) + (1-\theta)f(\vec{y})
    \end{equation}
\end{definition}

\begin{definition}[L-Smooth Function]
    A differentiable function $f: \R^n \to \R$ is \textit{L-smooth} if its gradient is Lipschitz continuous with constant $L > 0$:
    \begin{equation}
        \norm{\nabla f(\vec{x}) - \nabla f(\vec{y})}_2 \leq L\norm{\vec{x} - \vec{y}}_2 \quad \forall \vec{x}, \vec{y} \in \R^n
    \end{equation}
\end{definition}

\begin{theorem}[Descent Lemma]\label{thm:descent}
    If $f: \R^n \to \R$ is convex and L-smooth, then for all $\vec{x}, \vec{y} \in \R^n$:
    \begin{equation}
        f(\vec{y}) \leq f(\vec{x}) + \inner{\nabla f(\vec{x})}{\vec{y} - \vec{x}} + \frac{L}{2}\norm{\vec{y} - \vec{x}}_2^2
        \label{eq:descent_lemma}
    \end{equation}
\end{theorem}

\begin{proof}[Proof Sketch]
    The proof leverages the fundamental theorem of calculus and Lipschitz continuity:
    \begin{align}
        f(\vec{y}) - f(\vec{x}) & = \int_0^1 \inner{\nabla f(\vec{x} + t(\vec{y} - \vec{x}))}{\vec{y} - \vec{x}} dt                                                                    \\
                                & = \inner{\nabla f(\vec{x})}{\vec{y} - \vec{x}} + \int_0^1 \inner{\nabla f(\vec{x} + t(\vec{y} - \vec{x})) - \nabla f(\vec{x})}{\vec{y} - \vec{x}} dt
    \end{align}
    Applying Cauchy-Schwarz and the L-smoothness property completes the proof.
\end{proof}

\subsection{Majorization-Minimization Framework}

The majorization-minimization (MM) principle provides a general framework for iterative optimization:

\begin{definition}[Majorizer]
    A function $Q_{\vec{x}}(\vec{z})$ is a \textit{majorizer} of $f$ at point $\vec{x}$ if:
    \begin{enumerate}
        \item $Q_{\vec{x}}(\vec{z}) \geq f(\vec{z})$ for all $\vec{z} \in \operatorname{dom}(f)$ (global upper bound)
        \item $Q_{\vec{x}}(\vec{x}) = f(\vec{x})$ (tangency condition)
    \end{enumerate}
\end{definition}

For L-smooth convex functions, the descent lemma immediately provides a quadratic majorizer:
\begin{equation}
    Q_{\vec{x}}(\vec{z}) = f(\vec{x}) + \inner{\nabla f(\vec{x})}{\vec{z} - \vec{x}} + \frac{L}{2}\norm{\vec{z} - \vec{x}}_2^2
    \label{eq:quadratic_majorizer}
\end{equation}

\begin{proposition}[MM Algorithm Convergence]
    The iterative scheme:
    \begin{equation}
        \vec{x}_{k+1} = \argmin_{\vec{z}} Q_{\vec{x}_k}(\vec{z})
    \end{equation}
    generates a sequence $\{\vec{x}_k\}$ with monotonically decreasing function values: $f(\vec{x}_{k+1}) \leq f(\vec{x}_k)$.
\end{proposition}

% NOTE: The MM framework generalizes gradient descent and enables handling of non-smooth terms

\newpage
%==============================================================================
\section{Proximal Gradient Methods}
%==============================================================================

\subsection{The Composite Optimization Problem}

Consider the general composite optimization problem:
\begin{equation}
    \min_{\vec{x} \in \R^n} F(\vec{x}) := f(\vec{x}) + g(\vec{x})
    \label{eq:composite}
\end{equation}
where:
\begin{itemize}
    \item $f: \R^n \to \R$ is convex and L-smooth (differentiable component)
    \item $g: \R^n \to \R \cup \{+\infty\}$ is convex but possibly non-smooth (regularizer)
\end{itemize}

\subsection{Proximal Mapping: The Key Innovation}

\begin{definition}[Proximal Mapping]\label{def:prox}
    The \textit{proximal mapping} of a convex function $g$ at point $\vec{v}$ with parameter $\gamma > 0$ is:
    \begin{equation}
        \prox_{\gamma g}(\vec{v}) := \argmin_{\vec{z} \in \R^n} \left\{g(\vec{z}) + \frac{1}{2\gamma}\norm{\vec{z} - \vec{v}}_2^2\right\}
        \label{eq:prox_def}
    \end{equation}
\end{definition}

\begin{remark}
    The proximal mapping balances two objectives:
    \begin{enumerate}
        \item Minimizing the function $g$ (the original objective)
        \item Staying close to the reference point $\vec{v}$ (stability/regularization)
    \end{enumerate}
    The parameter $\gamma$ controls this trade-off: larger $\gamma$ emphasizes minimizing $g$, while smaller $\gamma$ keeps the solution closer to $\vec{v}$.
\end{remark}

\subsection{Derivation of the Proximal Gradient Algorithm}

Starting from the majorizer \eqref{eq:quadratic_majorizer} for the smooth component $f$:

\begin{align}
    F(\vec{z}) & = f(\vec{z}) + g(\vec{z})                                                                                             \\
               & \leq f(\vec{x}) + \inner{\nabla f(\vec{x})}{\vec{z} - \vec{x}} + \frac{L}{2}\norm{\vec{z} - \vec{x}}_2^2 + g(\vec{z})
\end{align}

Setting $\gamma = 1/L$ and minimizing the right-hand side:
\begin{align}
    \vec{x}_{k+1} & = \argmin_{\vec{z}} \left\{f(\vec{x}_k) + \inner{\nabla f(\vec{x}_k)}{\vec{z} - \vec{x}_k} + \frac{1}{2\gamma}\norm{\vec{z} - \vec{x}_k}_2^2 + g(\vec{z})\right\} \\
                  & = \argmin_{\vec{z}} \left\{\frac{1}{2\gamma}\norm{\vec{z} - \vec{x}_k}_2^2 + \inner{\nabla f(\vec{x}_k)}{\vec{z}} + g(\vec{z})\right\}
    \intertext{Completing the square:}
                  & = \argmin_{\vec{z}} \left\{\frac{1}{2\gamma}\norm{\vec{z} - (\vec{x}_k - \gamma\nabla f(\vec{x}_k))}_2^2 + g(\vec{z})\right\}                                     \\
                  & = \prox_{\gamma g}(\vec{x}_k - \gamma\nabla f(\vec{x}_k))
\end{align}

\begin{theorem}[Proximal Gradient Algorithm]
    The iterative scheme:
    \begin{equation}
        \vec{x}_{k+1} = \prox_{\gamma g}(\vec{x}_k - \gamma\nabla f(\vec{x}_k))
        \label{eq:prox_grad}
    \end{equation}
    converges to a minimizer of $F = f + g$ when $f$ is convex and L-smooth, $g$ is convex, and $0 < \gamma \leq 1/L$.
\end{theorem}

% NOTE: This algorithm decouples the smooth and non-smooth components elegantly

\newpage
%==============================================================================
\section{Computing the Proximal Mapping of the $\ell_1$ Norm}
%==============================================================================

\subsection{Problem Formulation}

For the $\ell_1$ regularization term $g(\vec{x}) = \lambda\norm{\vec{x}}_1$, we need to compute:
\begin{equation}
    \prox_{\gamma\lambda\norm{\cdot}_1}(\vec{v}) = \argmin_{\vec{z}} \left\{\lambda\norm{\vec{z}}_1 + \frac{1}{2\gamma}\norm{\vec{z} - \vec{v}}_2^2\right\}
    \label{eq:l1_prox_problem}
\end{equation}

\subsection{Separability and Component-wise Solution}

A crucial observation is that both terms in \eqref{eq:l1_prox_problem} are separable:
\begin{align}
    \lambda\norm{\vec{z}}_1                       & = \lambda\sum_{i=1}^n |z_i|                   \\
    \frac{1}{2\gamma}\norm{\vec{z} - \vec{v}}_2^2 & = \frac{1}{2\gamma}\sum_{i=1}^n (z_i - v_i)^2
\end{align}

This separability allows us to solve for each component independently:
\begin{equation}
    [\prox_{\gamma\lambda\norm{\cdot}_1}(\vec{v})]_i = \argmin_{z_i \in \R} \left\{\lambda|z_i| + \frac{1}{2\gamma}(z_i - v_i)^2\right\}
    \label{eq:scalar_problem}
\end{equation}

\subsection{Solving the Scalar Problem}

For the scalar optimization problem \eqref{eq:scalar_problem}, we consider three cases:

\subsubsection{Case 1: $z_i > 0$}
The objective becomes:
\begin{equation}
    h(z_i) = \lambda z_i + \frac{1}{2\gamma}(z_i - v_i)^2
\end{equation}
Taking the derivative and setting to zero:
\begin{align}
    \frac{\partial h}{\partial z_i} & = \lambda + \frac{1}{\gamma}(z_i - v_i) = 0 \\
    \Rightarrow z_i^*               & = v_i - \gamma\lambda
\end{align}
This solution is valid only if $z_i^* > 0$, i.e., $v_i > \gamma\lambda$.

\subsubsection{Case 2: $z_i < 0$}
The objective becomes:
\begin{equation}
    h(z_i) = -\lambda z_i + \frac{1}{2\gamma}(z_i - v_i)^2
\end{equation}
Similarly:
\begin{align}
    \frac{\partial h}{\partial z_i} & = -\lambda + \frac{1}{\gamma}(z_i - v_i) = 0 \\
    \Rightarrow z_i^*               & = v_i + \gamma\lambda
\end{align}
Valid when $z_i^* < 0$, i.e., $v_i < -\gamma\lambda$.

\subsubsection{Case 3: $z_i = 0$}
By the subdifferential optimality condition, $0 \in \partial h(0)$:
\begin{equation}
    0 \in \lambda[-1,1] + \frac{1}{\gamma}(0 - v_i)
\end{equation}
This holds when $|v_i| \leq \gamma\lambda$.

\subsection{The Soft Thresholding Operator}

Combining all cases yields the \textit{soft thresholding operator}:

\begin{theorem}[Soft Thresholding]
    The proximal mapping of the $\ell_1$ norm is given component-wise by:
    \begin{equation}
        [\prox_{\gamma\lambda\norm{\cdot}_1}(\vec{v})]_i = \soft_{\gamma\lambda}(v_i) := \sign(v_i)\max\{|v_i| - \gamma\lambda, 0\}
        \label{eq:soft_threshold}
    \end{equation}
    Equivalently:
    \begin{equation}
        \soft_{\gamma\lambda}(v_i) = \begin{cases}
            v_i - \gamma\lambda & \text{if } v_i > \gamma\lambda      \\
            0                   & \text{if } |v_i| \leq \gamma\lambda \\
            v_i + \gamma\lambda & \text{if } v_i < -\gamma\lambda
        \end{cases}
    \end{equation}
\end{theorem}

\begin{remark}[Comparison with Hard Thresholding]
    The soft thresholding operator differs fundamentally from hard thresholding:
    \begin{itemize}
        \item \textbf{Hard thresholding}: $\mathcal{H}_\tau(v) = v \cdot \mathbb{1}_{|v| > \tau}$ (discontinuous, preserves large values)
        \item \textbf{Soft thresholding}: $\soft_\tau(v) = \sign(v)\max\{|v|-\tau, 0\}$ (continuous, shrinks all values)
    \end{itemize}
\end{remark}

% NOTE: Soft thresholding induces sparsity while maintaining continuity

\newpage
%==============================================================================
\section{The ISTA Algorithm: Iterative Soft Thresholding}
%==============================================================================

\subsection{Algorithm Formulation}

Combining the proximal gradient framework with the soft thresholding operator yields:

\begin{theorem}[ISTA for Basis Pursuit Denoising]
    The Iterative Soft Thresholding Algorithm (ISTA) for solving:
    \begin{equation}
        \min_{\vec{x}} \frac{1}{2}\norm{\vec{D}\vec{x} - \vec{y}}_2^2 + \lambda\norm{\vec{x}}_1
    \end{equation}
    is given by:
    \begin{equation}
        \boxed{\vec{x}_{k+1} = \soft_{\gamma\lambda}(\vec{x}_k - \gamma\vec{D}^T(\vec{D}\vec{x}_k - \vec{y}))}
        \label{eq:ista}
    \end{equation}
    where $\gamma \in (0, 1/\norm{\vec{D}^T\vec{D}}_2]$ is the step size.
\end{theorem}

\subsection{Algorithm Implementation}

\begin{example}[ISTA Implementation]
    The algorithm can be implemented efficiently as:
    \begin{enumerate}
        \item \textbf{Precomputation}:
              \begin{itemize}
                  \item Compute $\vec{D}^T\vec{D}$ and $\vec{D}^T\vec{y}$ (one-time cost)
                  \item Estimate $L = \norm{\vec{D}^T\vec{D}}_2$ via power iteration
              \end{itemize}

        \item \textbf{Iteration}: For $k = 0, 1, 2, \ldots$
              \begin{align}
                  \vec{g}_k     & = \vec{D}^T(\vec{D}\vec{x}_k - \vec{y}) \quad \text{(gradient computation)} \\
                  \vec{z}_k     & = \vec{x}_k - \gamma\vec{g}_k \quad \text{(gradient descent step)}          \\
                  \vec{x}_{k+1} & = \soft_{\gamma\lambda}(\vec{z}_k) \quad \text{(soft thresholding)}
              \end{align}
    \end{enumerate}
\end{example}

\subsection{Convergence Analysis}

\begin{theorem}[ISTA Convergence Rate]
    Under the conditions:
    \begin{itemize}
        \item $f(\vec{x}) = \frac{1}{2}\norm{\vec{D}\vec{x} - \vec{y}}_2^2$ is convex and L-smooth
        \item $g(\vec{x}) = \lambda\norm{\vec{x}}_1$ is convex
        \item Step size $\gamma \in (0, 1/L]$
    \end{itemize}
    ISTA converges with rate:
    \begin{equation}
        F(\vec{x}_k) - F(\vec{x}^*) \leq \frac{\norm{\vec{x}_0 - \vec{x}^*}_2^2}{2\gamma k}
    \end{equation}
    where $\vec{x}^*$ is an optimal solution.
\end{theorem}

\subsection{Extensions and Generalizations}

The proximal gradient framework extends naturally to other regularizers:

\begin{example}[Group Lasso]
    For group sparsity with $g(\vec{x}) = \sum_{g \in \mathcal{G}} \lambda_g\norm{\vec{x}_g}_2$:
    \begin{equation}
        [\prox_{\gamma g}(\vec{v})]_g = \max\left\{1 - \frac{\gamma\lambda_g}{\norm{\vec{v}_g}_2}, 0\right\}\vec{v}_g
    \end{equation}
\end{example}

\begin{example}[Nuclear Norm]
    For low-rank matrix recovery with $g(\vec{X}) = \lambda\norm{\vec{X}}_*$:
    \begin{equation}
        \prox_{\gamma\lambda\norm{\cdot}_*}(\vec{V}) = \vec{U}\soft_{\gamma\lambda}(\vec{\Sigma})\vec{V}^T
    \end{equation}
    where $\vec{V} = \vec{U}\vec{\Sigma}\vec{V}^T$ is the SVD.
\end{example}

% NOTE: The proximal gradient framework is remarkably general and powerful

\newpage
%==============================================================================
\section{Practical Considerations and Advanced Topics}
%==============================================================================

\subsection{Acceleration: FISTA}

The Fast Iterative Soft Thresholding Algorithm (FISTA) improves convergence from $O(1/k)$ to $O(1/k^2)$:

\begin{align}
    \vec{y}_{k+1} & = \vec{x}_k + \frac{t_k - 1}{t_{k+1}}(\vec{x}_k - \vec{x}_{k-1})                         \\
    \vec{x}_{k+1} & = \soft_{\gamma\lambda}(\vec{y}_{k+1} - \gamma\vec{D}^T(\vec{D}\vec{y}_{k+1} - \vec{y})) \\
    t_{k+1}       & = \frac{1 + \sqrt{1 + 4t_k^2}}{2}
\end{align}

\subsection{Adaptive Step Size Selection}

In practice, the Lipschitz constant $L$ may be unknown or conservative. Backtracking line search provides an adaptive solution:

\begin{enumerate}
    \item Start with $\gamma = \gamma_0$ (e.g., $\gamma_0 = 1$)
    \item While the descent condition is violated:
          \begin{equation}
              f(\vec{x}_{k+1}) > f(\vec{x}_k) + \inner{\nabla f(\vec{x}_k)}{\vec{x}_{k+1} - \vec{x}_k} + \frac{1}{2\gamma}\norm{\vec{x}_{k+1} - \vec{x}_k}_2^2
          \end{equation}
          reduce $\gamma \leftarrow \beta\gamma$ (typically $\beta = 0.5$)
\end{enumerate}

\subsection{Warm Starting and Continuation}

For solving a sequence of related problems (e.g., regularization path):
\begin{itemize}
    \item \textbf{Warm starting}: Initialize $\vec{x}_0$ for $\lambda_i$ using solution from $\lambda_{i-1}$
    \item \textbf{Continuation}: Solve for decreasing sequence $\lambda_1 > \lambda_2 > \cdots > \lambda_{\text{target}}$
\end{itemize}

\subsection{Stopping Criteria}

Practical termination conditions include:
\begin{enumerate}
    \item \textbf{Relative change}: $\frac{\norm{\vec{x}_{k+1} - \vec{x}_k}_2}{\norm{\vec{x}_k}_2} < \epsilon_{\text{tol}}$
    \item \textbf{Gradient magnitude}: $\norm{\vec{x}_k - \prox_{\gamma g}(\vec{x}_k - \gamma\nabla f(\vec{x}_k))}_2 < \epsilon_{\text{tol}}$
    \item \textbf{Duality gap}: For problems with known dual formulation
\end{enumerate}

%==============================================================================
\section{Conclusion}
%==============================================================================

The proximal gradient method elegantly handles composite optimization problems by:
\begin{enumerate}
    \item Separating smooth and non-smooth components
    \item Applying gradient descent to the smooth part
    \item Using proximal mappings for the non-smooth part
\end{enumerate}

For $\ell_1$-regularized problems, this yields the efficient ISTA algorithm, where each iteration consists of:
\begin{itemize}
    \item A gradient descent step (handling data fidelity)
    \item A soft thresholding operation (inducing sparsity)
\end{itemize}

The framework extends naturally to numerous other regularizers and has become a cornerstone of modern optimization in machine learning and signal processing.

\begin{remark}[Historical Note]
    The development of proximal methods traces back to Moreau (1962) and Rockafellar (1976), but their application to sparse recovery problems emerged prominently with the work of Daubechies, Defrise, and De Mol (2004) on iterative thresholding, followed by Beck and Teboulle's FISTA (2009).
\end{remark}

% NOTE: This framework has revolutionized sparse optimization and continues to inspire new algorithms

\end{document}