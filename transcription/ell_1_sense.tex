\documentclass[12pt]{article}
\usepackage[margin=1in]{geometry}
\usepackage{amsmath, amsthm, amssymb, mathtools, mathrsfs}
\usepackage{enumitem, graphicx, hyperref, booktabs, array}
\usepackage{algorithm}
\usepackage{algorithmicx}
\usepackage[mathscr]{euscript}
\usepackage[T1]{fontenc}
\usepackage{microtype}
\usepackage{tikz}
\usetikzlibrary{shapes.geometric, arrows}

\hypersetup{colorlinks=true, linkcolor=blue, citecolor=red}

% NOTE: Custom commands for consistent notation
\renewcommand{\vec}[1]{\mathbf{#1}}
\DeclareMathOperator{\argmin}{argmin}
\DeclareMathOperator{\argmax}{argmax}
\DeclareMathOperator{\Var}{Var}
\DeclareMathOperator{\sign}{sign}
\DeclareMathOperator{\prox}{prox}
\newcommand{\norm}[1]{\lVert#1\rVert}
\newcommand{\abs}[1]{\lvert#1\rvert}
\newcommand{\R}{\mathbb{R}}
\newcommand{\N}{\mathbb{N}}

% NOTE: Theorem environments
\theoremstyle{definition}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{example}[theorem]{Example}
\newtheorem{remark}[theorem]{Remark}

\title{L1 Optimization and Sparse Coding:\\ From Sparsity-Promoting Norms to Convex Optimization}
\author{Lecture Notes}
\date{\today}

\begin{document}

\maketitle

\tableofcontents
\newpage

% NOTE: This document expands on the sparse coding lecture with enhanced mathematical rigor and comprehensive derivations

\section{Introduction to Sparsity-Promoting Norms}

The quest for sparse representations in signal processing and machine learning has led to the development of various sparsity-promoting norms. This lecture transitions from the intuitive but computationally intractable $\ell_0$ norm to more practical alternatives, particularly the $\ell_1$ norm, which offers a favorable balance between sparsity promotion and computational tractability.

\subsection{Motivation: From $\ell_0$ to $\ell_1$}

The $\ell_0$ norm, defined as the number of non-zero components in a vector, provides the most intuitive measure of sparsity. However, optimization problems involving the $\ell_0$ norm are NP-hard due to their combinatorial nature. This computational intractability necessitates the exploration of alternative sparsity-promoting norms that maintain favorable optimization properties.

\begin{definition}[Sparsity Measure]
    For a vector $\vec{x} \in \R^n$, the $\ell_0$ norm is defined as:
    \begin{equation}
        \norm{\vec{x}}_0 = \sum_{i=1}^n \mathbf{1}_{x_i \neq 0}
    \end{equation}
    where $\mathbf{1}_{x_i \neq 0}$ is the indicator function that equals 1 if $x_i \neq 0$ and 0 otherwise.
\end{definition}

The challenge lies in finding norms that promote sparsity while yielding tractable optimization problems. This leads us to consider the family of $\ell_p$ norms for various values of $p$.

\newpage

\section{Mathematical Framework: $\ell_p$ Norms}

\subsection{Definition and Properties}

The $\ell_p$ norm extends the familiar concept of the Euclidean norm to a broader family of norms parametrized by $p \geq 1$.

\begin{definition}[$\ell_p$ Norm]
    For a vector $\vec{x} \in \R^n$ and $p \geq 1$, the $\ell_p$ norm is defined as:
    \begin{equation}
        \norm{\vec{x}}_p = \left(\sum_{i=1}^n \abs{x_i}^p\right)^{1/p}
    \end{equation}
\end{definition}

\begin{example}[Common $\ell_p$ Norms]
    \begin{align}
        \norm{\vec{x}}_1      & = \sum_{i=1}^n \abs{x_i} \quad \text{(Manhattan norm)}         \\
        \norm{\vec{x}}_2      & = \sqrt{\sum_{i=1}^n x_i^2} \quad \text{(Euclidean norm)}      \\
        \norm{\vec{x}}_\infty & = \max_{1 \leq i \leq n} \abs{x_i} \quad \text{(Maximum norm)}
    \end{align}
\end{example}

\subsection{The Case $p < 1$: Quasi-Norms}

For $0 < p < 1$, the expression $\left(\sum_{i=1}^n \abs{x_i}^p\right)^{1/p}$ does not satisfy the triangle inequality and thus is not a norm in the mathematical sense. These are referred to as quasi-norms.

\begin{theorem}[Triangle Inequality Failure for $p < 1$]
    For $0 < p < 1$, the triangle inequality $\norm{\vec{x} + \vec{y}}_p \leq \norm{\vec{x}}_p + \norm{\vec{y}}_p$ does not hold in general.
\end{theorem}

\begin{proof}[Proof Sketch]
    Consider the counterexample with $\vec{x} = (1, 0)^T$ and $\vec{y} = (0, 1)^T$ in $\R^2$. For $p < 1$:
    \begin{align}
        \norm{\vec{x} + \vec{y}}_p          & = \norm{(1, 1)^T}_p = (1^p + 1^p)^{1/p} = 2^{1/p} \\
        \norm{\vec{x}}_p + \norm{\vec{y}}_p & = 1^{1/p} + 1^{1/p} = 2
    \end{align}
    Since $1/p > 1$ for $p < 1$, we have $2^{1/p} > 2$, violating the triangle inequality.
\end{proof}

\newpage

\section{Geometric Interpretation: Unit Balls and Sparsity}

\subsection{Visualization of $\ell_p$ Unit Balls}

The geometric properties of $\ell_p$ norms can be understood through their unit balls, defined as:
\begin{equation}
    B_p = \{\vec{x} \in \R^n : \norm{\vec{x}}_p \leq 1\}
\end{equation}

\begin{figure}[h]
    \centering
    % NOTE: In actual LaTeX, you would include a TikZ diagram here
    \caption{Unit balls for various $\ell_p$ norms in $\R^2$. The $\ell_1$ ball forms a diamond shape, while $\ell_2$ forms a circle, and $\ell_\infty$ forms a square.}
\end{figure}

\subsection{Sparsity Promotion Through Geometric Analysis}

The sparsity-promoting properties of different norms can be understood through a geometric optimization perspective. Consider the constrained optimization problem:
\begin{equation}
    \min_{\vec{x}} \norm{\vec{x}}_p \quad \text{subject to} \quad \mathbf{A}\vec{x} = \vec{b}
\end{equation}

where $\mathbf{A} \in \R^{m \times n}$ with $m < n$ (underdetermined system).

\begin{theorem}[Geometric Sparsity Argument]
    For underdetermined linear systems, the $\ell_1$ norm promotes sparse solutions more effectively than the $\ell_2$ norm due to the angular structure of the $\ell_1$ unit ball.
\end{theorem}

\begin{proof}[Geometric Interpretation]
    The solution is found by inflating the $\ell_p$ unit ball until it touches the solution set (an affine subspace). The $\ell_1$ ball's diamond shape with sharp corners along the coordinate axes makes it more likely to intersect the solution set at a sparse point (where many coordinates are zero) compared to the smooth $\ell_2$ ball.
\end{proof}

\newpage

\section{Convex Optimization Theory}

\subsection{Convex Sets and Functions}

\begin{definition}[Convex Set]
    A set $S \subseteq \R^n$ is convex if and only if for any two points $\vec{x}, \vec{y} \in S$ and any $\alpha \in [0, 1]$:
    \begin{equation}
        \alpha \vec{x} + (1 - \alpha) \vec{y} \in S
    \end{equation}
\end{definition}

\begin{definition}[Convex Function]
    A function $f: S \to \R$ defined on a convex set $S \subseteq \R^n$ is convex if for any $\vec{x}, \vec{y} \in S$ and $\alpha \in [0, 1]$:
    \begin{equation}
        f(\alpha \vec{x} + (1 - \alpha) \vec{y}) \leq \alpha f(\vec{x}) + (1 - \alpha) f(\vec{y})
    \end{equation}
\end{definition}

\subsection{Convexity of $\ell_p$ Norms}

\begin{theorem}[Convexity of $\ell_p$ Norms]
    For $p \geq 1$, the $\ell_p$ norm is a convex function on $\R^n$.
\end{theorem}

\begin{proof}
    For any $\vec{x}, \vec{y} \in \R^n$ and $\alpha \in [0, 1]$, we need to show:
    \begin{equation}
        \norm{\alpha \vec{x} + (1 - \alpha) \vec{y}}_p \leq \alpha \norm{\vec{x}}_p + (1 - \alpha) \norm{\vec{y}}_p
    \end{equation}

    Using the triangle inequality for norms:
    \begin{align}
        \norm{\alpha \vec{x} + (1 - \alpha) \vec{y}}_p & \leq \norm{\alpha \vec{x}}_p + \norm{(1 - \alpha) \vec{y}}_p \\
                                                       & = \alpha \norm{\vec{x}}_p + (1 - \alpha) \norm{\vec{y}}_p
    \end{align}
    where the last equality uses the homogeneity property of norms.
\end{proof}

\subsection{Fundamental Optimization Result}

\begin{theorem}[Global Optimality in Convex Optimization]
    For a convex optimization problem, any local minimum is also a global minimum.
\end{theorem}

This result is crucial for sparse coding applications, as it guarantees that any convergent optimization algorithm will find the globally optimal solution.

\newpage

\section{The $\ell_1$ Optimization Problem}

\subsection{Problem Formulation}

The $\ell_1$ optimization problem for sparse coding can be formulated in two equivalent ways:

\begin{definition}[Constrained $\ell_1$ Problem (P1)]
    \begin{equation}
        \min_{\vec{x}} \norm{\vec{x}}_1 \quad \text{subject to} \quad \norm{\mathbf{A}\vec{x} - \vec{b}}_2 \leq \epsilon
    \end{equation}
\end{definition}

\begin{definition}[Regularized $\ell_1$ Problem (P2)]
    \begin{equation}
        \min_{\vec{x}} \frac{1}{2}\norm{\mathbf{A}\vec{x} - \vec{b}}_2^2 + \lambda \norm{\vec{x}}_1
    \end{equation}
\end{definition}

\subsection{Connection to LASSO}

The regularized formulation (P2) is known in statistics as the Least Absolute Shrinkage and Selection Operator (LASSO), introduced by Robert Tibshirani.

\begin{remark}[LASSO vs. Sparse Coding]
    While LASSO and sparse coding share the same mathematical formulation, they operate in different contexts:
    \begin{itemize}
        \item \textbf{LASSO}: Overdetermined systems ($m > n$) for variable selection
        \item \textbf{Sparse Coding}: Underdetermined systems ($m < n$) for signal representation
    \end{itemize}
\end{remark}

\subsection{Problem Components Analysis}

\subsubsection{Data Fidelity Term}

The term $\frac{1}{2}\norm{\mathbf{A}\vec{x} - \vec{b}}_2^2$ serves as the data fidelity term, ensuring that the solution $\vec{x}$ produces a reconstruction $\mathbf{A}\vec{x}$ that is close to the observed signal $\vec{b}$.

\begin{proposition}[Properties of Data Fidelity Term]
    The data fidelity term $g(\vec{x}) = \frac{1}{2}\norm{\mathbf{A}\vec{x} - \vec{b}}_2^2$ is:
    \begin{enumerate}
        \item Convex (as a composition of convex functions)
        \item Differentiable with gradient $\nabla g(\vec{x}) = \mathbf{A}^T(\mathbf{A}\vec{x} - \vec{b})$
        \item Strongly convex if $\mathbf{A}$ has full column rank
    \end{enumerate}
\end{proposition}

\subsubsection{Regularization Term}

The term $\lambda \norm{\vec{x}}_1$ acts as a regularization term, promoting sparsity in the solution.

\begin{proposition}[Properties of $\ell_1$ Regularization]
    The regularization term $h(\vec{x}) = \norm{\vec{x}}_1$ is:
    \begin{enumerate}
        \item Convex
        \item Non-differentiable at $x_i = 0$ for any component $i$
        \item Promotes sparsity through its geometric properties
    \end{enumerate}
\end{proposition}

\newpage

\section{Optimization Theory for Non-Differentiable Functions}

\subsection{The Descent Lemma}

For smooth convex functions, we can construct quadratic majorizers that facilitate optimization.

\begin{lemma}[Descent Lemma]
    Let $f: \R^n \to \R$ be a convex, differentiable function with Lipschitz continuous gradient. Then for any $\vec{x}_k \in \R^n$, there exists $L > 0$ such that:
    \begin{equation}
        f(\vec{x}) \leq Q_L(\vec{x}; \vec{x}_k) = f(\vec{x}_k) + \nabla f(\vec{x}_k)^T(\vec{x} - \vec{x}_k) + \frac{L}{2}\norm{\vec{x} - \vec{x}_k}_2^2
    \end{equation}
    for all $\vec{x} \in \R^n$.
\end{lemma}

\subsection{Majorization-Minimization Approach}

\begin{definition}[Majorization-Minimization Algorithm]
    Given a convex function $f$, the majorization-minimization approach generates a sequence $\{\vec{x}_k\}$ by:
    \begin{align}
        \vec{x}_{k+1} & = \argmin_{\vec{x}} Q_L(\vec{x}; \vec{x}_k)                                                                                            \\
                      & = \argmin_{\vec{x}} \left[f(\vec{x}_k) + \nabla f(\vec{x}_k)^T(\vec{x} - \vec{x}_k) + \frac{L}{2}\norm{\vec{x} - \vec{x}_k}_2^2\right]
    \end{align}
\end{definition}

\subsection{Gradient Descent Derivation}

Minimizing the majorizer $Q_L(\vec{x}; \vec{x}_k)$ with respect to $\vec{x}$:
\begin{align}
    \nabla_{\vec{x}} Q_L(\vec{x}; \vec{x}_k) & = \nabla f(\vec{x}_k) + L(\vec{x} - \vec{x}_k) = 0 \\
    \Rightarrow \vec{x}_{k+1}                & = \vec{x}_k - \frac{1}{L}\nabla f(\vec{x}_k)
\end{align}

This recovers the standard gradient descent update with step size $\gamma = 1/L$.

\begin{theorem}[Gradient Descent Convergence]
    For a convex, differentiable function $f$ with Lipschitz continuous gradient, the gradient descent algorithm converges to the global minimum.
\end{theorem}

\newpage

\section{Proximal Gradient Methods}

\subsection{Proximal Operator}

For the composite optimization problem $\min_{\vec{x}} f(\vec{x}) + g(\vec{x})$ where $f$ is smooth and $g$ is non-smooth, we introduce the proximal operator.

\begin{definition}[Proximal Operator]
    The proximal operator of a function $g$ with parameter $\lambda > 0$ is defined as:
    \begin{equation}
        \prox_{\lambda g}(\vec{v}) = \argmin_{\vec{x}} \left\{\frac{1}{2\lambda}\norm{\vec{x} - \vec{v}}_2^2 + g(\vec{x})\right\}
    \end{equation}
\end{definition}

\subsection{Proximal Gradient Algorithm}

% [Proximal Gradient Method]

\begin{algorithm}
    For the problem $\min_{\vec{x}} f(\vec{x}) + g(\vec{x})$:
    \begin{enumerate}
        \item Initialize $\vec{x}_0$
        \item For $k = 0, 1, 2, \ldots$:
              \begin{align}
                  \vec{y}_k     & = \vec{x}_k - \frac{1}{L}\nabla f(\vec{x}_k) \\
                  \vec{x}_{k+1} & = \prox_{\frac{1}{L}g}(\vec{y}_k)
              \end{align}
    \end{enumerate}
\end{algorithm}

\subsection{Proximal Operator of $\ell_1$ Norm}

\begin{theorem}[Soft Thresholding]
    The proximal operator of the $\ell_1$ norm is given by the soft thresholding operator:
    \begin{equation}
        [\prox_{\lambda \norm{\cdot}_1}(\vec{v})]_i = \sign(v_i) \max\{|v_i| - \lambda, 0\}
    \end{equation}
    where $\sign(v_i)$ is the sign function.
\end{theorem}

\begin{proof}
    The proximal operator problem becomes:
    \begin{equation}
        \min_{\vec{x}} \left\{\frac{1}{2\lambda}\norm{\vec{x} - \vec{v}}_2^2 + \norm{\vec{x}}_1\right\}
    \end{equation}
    This separates into $n$ independent scalar problems:
    \begin{equation}
        \min_{x_i} \left\{\frac{1}{2\lambda}(x_i - v_i)^2 + |x_i|\right\}
    \end{equation}
    The solution is the soft thresholding operator due to the subdifferential analysis of the absolute value function.
\end{proof}

\newpage

\section{Applications and Extensions}

\subsection{Signal Processing Applications}

The $\ell_1$ optimization framework finds extensive applications in:

\begin{itemize}
    \item \textbf{Compressed Sensing}: Recovering sparse signals from undersampled measurements
    \item \textbf{Image Denoising}: Removing noise while preserving important features
    \item \textbf{Feature Selection}: Identifying relevant variables in high-dimensional data
    \item \textbf{Dictionary Learning}: Learning overcomplete bases for signal representation
\end{itemize}

\subsection{Computational Considerations}

\begin{remark}[Algorithmic Efficiency]
    The proximal gradient method for $\ell_1$ optimization has several computational advantages:
    \begin{enumerate}
        \item Uses only first-order information (gradients)
        \item Scales well to high dimensions
        \item Produces sparse solutions automatically through soft thresholding
        \item Guaranteed global convergence for convex problems
    \end{enumerate}
\end{remark}

\subsection{Extensions to Other Norms}

The framework extends naturally to other sparsity-promoting norms:

\begin{itemize}
    \item \textbf{Group LASSO}: $\norm{\vec{x}}_{\text{group}} = \sum_g \norm{\vec{x}_g}_2$
    \item \textbf{Elastic Net}: $\alpha \norm{\vec{x}}_1 + (1-\alpha) \norm{\vec{x}}_2^2$
    \item \textbf{Total Variation}: $\norm{\nabla \vec{x}}_1$ for piecewise constant signals
\end{itemize}

\section{Conclusion}

The transition from $\ell_0$ to $\ell_1$ optimization represents a fundamental shift in sparse coding methodology. By leveraging the convex optimization framework, we obtain:

\begin{enumerate}
    \item \textbf{Computational Tractability}: Polynomial-time algorithms with global optimality guarantees
    \item \textbf{Sparsity Promotion}: Geometric properties that encourage sparse solutions
    \item \textbf{Theoretical Foundation}: Rigorous mathematical framework for analysis
    \item \textbf{Practical Effectiveness}: Wide applicability across signal processing domains
\end{enumerate}

The proximal gradient method provides an elegant solution to the non-differentiability challenge, enabling efficient optimization of $\ell_1$-regularized problems. This framework continues to be a cornerstone of modern sparse signal processing and machine learning applications.

\newpage

\section{Mathematical Appendix}

\subsection{Subdifferential Calculus}

For non-differentiable convex functions, we use the concept of subdifferentials:

\begin{definition}[Subdifferential]
    The subdifferential of a convex function $f$ at $\vec{x}$ is:
    \begin{equation}
        \partial f(\vec{x}) = \{\vec{g} \in \R^n : f(\vec{y}) \geq f(\vec{x}) + \vec{g}^T(\vec{y} - \vec{x}) \text{ for all } \vec{y}\}
    \end{equation}
\end{definition}

\subsection{Optimality Conditions}

\begin{theorem}[First-Order Optimality Condition]
    For the problem $\min_{\vec{x}} f(\vec{x}) + g(\vec{x})$ where $f$ is differentiable and $g$ is convex, $\vec{x}^*$ is optimal if and only if:
    \begin{equation}
        0 \in \nabla f(\vec{x}^*) + \partial g(\vec{x}^*)
    \end{equation}
\end{theorem}

\section{Glossary of Symbols}

\begin{tabular}{ll}
    $\vec{x}$           & Vector in $\R^n$         \\
    $\norm{\cdot}_p$    & $\ell_p$ norm            \\
    $\mathbf{A}$        & Dictionary matrix        \\
    $\vec{b}$           & Observed signal          \\
    $\lambda$           & Regularization parameter \\
    $\epsilon$          & Noise tolerance          \\
    $\prox_{\lambda g}$ & Proximal operator of $g$ \\
    $\partial f$        & Subdifferential of $f$   \\
    $L$                 & Lipschitz constant       \\
    $\gamma$            & Step size parameter      \\
\end{tabular}

\bibliographystyle{plain}
\bibliography{references}

\end{document}
