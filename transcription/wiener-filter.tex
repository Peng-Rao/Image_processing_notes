\documentclass[12pt]{article}
\usepackage[margin=1in]{geometry}
\usepackage{amsmath, amsthm, amssymb, mathtools, mathrsfs}
\usepackage{enumitem, graphicx, hyperref, booktabs, array}
\usepackage[mathscr]{euscript}
\usepackage[T1]{fontenc}
\usepackage{microtype}
\usepackage{algorithm}
\usepackage{algpseudocode}
\hypersetup{colorlinks=true, linkcolor=blue, citecolor=red}

% Custom commands and operators
\renewcommand{\vec}[1]{\mathbf{#1}}
\DeclareMathOperator{\Var}{Var}
\DeclareMathOperator{\E}{E}
\DeclareMathOperator{\MSE}{MSE}
\DeclareMathOperator{\argmin}{argmin}
\newcommand{\norm}[1]{\left\lVert#1\right\rVert}
\newcommand{\trans}[1]{#1^{\top}}

% Theorem environments
\theoremstyle{definition}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{remark}[theorem]{Remark}
\newtheorem{example}[theorem]{Example}

\title{The Wiener Filter: Optimal Linear Estimation for Image Denoising}
\author{Advanced Signal Processing Lecture Notes}
\date{\today}

\begin{document}

\maketitle

\begin{abstract}
    This document presents a comprehensive treatment of the Wiener filter as applied to image denoising problems. We develop the theoretical foundation from first principles, derive the optimal linear estimator under mean square error criterion, and discuss practical implementation strategies. The treatment encompasses both the classical frequency-domain formulation and modern patch-based approaches, with detailed mathematical derivations and practical considerations.
\end{abstract}

\tableofcontents
\newpage

% NOTE: This section establishes the fundamental problem setup and mathematical framework

\section{Introduction and Problem Formulation}
\label{sec:introduction}

The Wiener filter, named after Norbert Wiener, represents a cornerstone in optimal linear filtering theory. In the context of image processing, it addresses the fundamental problem of estimating a clean signal from its noisy observations under the constraint of linear operations.

\subsection{Mathematical Framework}
\label{subsec:framework}

Consider an image denoising problem where we observe a noisy image $\vec{y}$ related to the true image $\vec{x}$ through an additive noise model:

\begin{equation}
    \label{eq:noise_model}
    \vec{y} = \vec{x} + \vec{n}
\end{equation}

where:
\begin{itemize}[leftmargin=*]
    \item $\vec{x} \in \mathbb{R}^{N}$ represents the unknown clean image (vectorized)
    \item $\vec{y} \in \mathbb{R}^{N}$ denotes the observed noisy image
    \item $\vec{n} \in \mathbb{R}^{N}$ is additive white Gaussian noise with $\vec{n} \sim \mathcal{N}(\vec{0}, \sigma^2 \mathbf{I})$
\end{itemize}

% NOTE: The transform domain formulation is crucial for understanding frequency-selective filtering

\subsection{Transform Domain Representation}
\label{subsec:transform_domain}

To exploit the spectral characteristics of natural images, we apply an orthogonal transformation $\mathbf{T}$ to both the clean and noisy images:

\begin{align}
    \label{eq:transform_domain}
    \tilde{\vec{x}} & = \mathbf{T}\vec{x} \quad \text{(clean image coefficients)}                                     \\
    \tilde{\vec{y}} & = \mathbf{T}\vec{y} = \mathbf{T}\vec{x} + \mathbf{T}\vec{n} = \tilde{\vec{x}} + \tilde{\vec{n}}
\end{align}

where $\tilde{\vec{n}} = \mathbf{T}\vec{n}$ represents the transformed noise.

\begin{remark}
    For orthogonal transformations, the noise remains white with the same variance: $\tilde{\vec{n}} \sim \mathcal{N}(\vec{0}, \sigma^2 \mathbf{I})$, since $\E[\tilde{\vec{n}}\trans{\tilde{\vec{n}}}] = \E[\mathbf{T}\vec{n}\trans{\vec{n}}\trans{\mathbf{T}}] = \mathbf{T}\E[\vec{n}\trans{\vec{n}}]\trans{\mathbf{T}} = \sigma^2 \mathbf{T}\mathbf{I}\trans{\mathbf{T}} = \sigma^2 \mathbf{I}$.
\end{remark}

\newpage

% NOTE: This section develops the core theoretical results

\section{Optimal Linear Estimation Theory}
\label{sec:optimal_estimation}

The Wiener filter seeks the optimal linear estimator within a specified class of functions. We restrict our attention to coefficient-wise linear operations, which leads to a tractable optimization problem.

\subsection{Class of Estimators}
\label{subsec:estimator_class}

We consider estimators of the form:
\begin{equation}
    \label{eq:estimator_class}
    \hat{\tilde{x}}_i = \phi(\tilde{y}_i) = \alpha_i \tilde{y}_i, \quad i = 1, 2, \ldots, N
\end{equation}

where $\alpha_i$ are scalar coefficients to be determined optimally for each frequency component.

\begin{definition}[Coefficient-wise Linear Estimator]
    \label{def:coeff_linear}
    A coefficient-wise linear estimator is a function $\phi: \mathbb{R}^N \to \mathbb{R}^N$ such that:
    \begin{equation}
        \phi(\tilde{\vec{y}}) = \text{diag}(\alpha_1, \alpha_2, \ldots, \alpha_N) \tilde{\vec{y}}
    \end{equation}
    where each $\alpha_i$ operates independently on the $i$-th transform coefficient.
\end{definition}

\subsection{Mean Square Error Criterion}
\label{subsec:mse_criterion}

For a single coefficient $i$, we define the mean square error as:
\begin{equation}
    \label{eq:mse_single}
    \MSE_i(\alpha_i) = \E[(\hat{\tilde{x}}_i - \tilde{x}_i)^2] = \E[(\alpha_i \tilde{y}_i - \tilde{x}_i)^2]
\end{equation}

% NOTE: The following derivation shows the complete optimization process

\subsection{Derivation of Optimal Coefficients}
\label{subsec:optimal_derivation}

\begin{theorem}[Optimal Wiener Filter Coefficients]
    \label{thm:optimal_wiener}
    The optimal coefficient $\alpha_i^*$ that minimizes the mean square error $\MSE_i(\alpha_i)$ is given by:
    \begin{equation}
        \label{eq:optimal_alpha}
        \alpha_i^* = \frac{\sigma_{\tilde{x}_i}^2}{\sigma_{\tilde{x}_i}^2 + \sigma^2}
    \end{equation}
    where $\sigma_{\tilde{x}_i}^2 = \E[\tilde{x}_i^2]$ is the variance of the $i$-th clean coefficient.
\end{theorem}

\begin{proof}
    Expanding the mean square error:
    \begin{align}
        \MSE_i(\alpha_i)
         & = \E[(\alpha_i \tilde{y}_i - \tilde{x}_i)^2]                                                                              \\
         & = \E[(\alpha_i(\tilde{x}_i + \tilde{n}_i) - \tilde{x}_i)^2]                                                               \\
         & = \E[(\alpha_i \tilde{x}_i + \alpha_i \tilde{n}_i - \tilde{x}_i)^2]                                                       \\
         & = \E[((\alpha_i - 1)\tilde{x}_i + \alpha_i \tilde{n}_i)^2]                                                                \\
        \intertext{Expanding the quadratic expression:}
         & = \E[(\alpha_i - 1)^2 \tilde{x}_i^2 + 2(\alpha_i - 1)\alpha_i \tilde{x}_i \tilde{n}_i + \alpha_i^2 \tilde{n}_i^2]         \\
         & = (\alpha_i - 1)^2 \E[\tilde{x}_i^2] + 2(\alpha_i - 1)\alpha_i \E[\tilde{x}_i \tilde{n}_i] + \alpha_i^2 \E[\tilde{n}_i^2] \\
        \intertext{Since the clean signal and noise are independent, $\E[\tilde{x}_i \tilde{n}_i] = 0$:}
         & = (\alpha_i - 1)^2 \sigma_{\tilde{x}_i}^2 + \alpha_i^2 \sigma^2                                                           \\
         & = \alpha_i^2 \sigma_{\tilde{x}_i}^2 - 2\alpha_i \sigma_{\tilde{x}_i}^2 + \sigma_{\tilde{x}_i}^2 + \alpha_i^2 \sigma^2     \\
         & = \alpha_i^2(\sigma_{\tilde{x}_i}^2 + \sigma^2) - 2\alpha_i \sigma_{\tilde{x}_i}^2 + \sigma_{\tilde{x}_i}^2               \\
        \intertext{To find the minimum, we differentiate with respect to $\alpha_i$ and set equal to zero:}
        \frac{d\MSE_i}{d\alpha_i}
         & = 2\alpha_i(\sigma_{\tilde{x}_i}^2 + \sigma^2) - 2\sigma_{\tilde{x}_i}^2 = 0                                              \\
        \alpha_i^*
         & = \frac{\sigma_{\tilde{x}_i}^2}{\sigma_{\tilde{x}_i}^2 + \sigma^2}
    \end{align}
\end{proof}

\newpage

% NOTE: This section discusses the practical challenges and solutions

\section{Practical Implementation Challenges}
\label{sec:practical_challenges}

\subsection{The Fundamental Problem}
\label{subsec:fundamental_problem}

The optimal Wiener filter coefficient in Equation~\ref{eq:optimal_alpha} depends on the unknown signal variance $\sigma_{\tilde{x}_i}^2$. This creates a circular dependency:

\begin{itemize}[leftmargin=*]
    \item To apply the Wiener filter, we need $\sigma_{\tilde{x}_i}^2$
    \item To estimate $\sigma_{\tilde{x}_i}^2$, we need the clean signal $\tilde{x}_i$
    \item The clean signal is exactly what we're trying to estimate
\end{itemize}

\subsection{Empirical Wiener Filter Approach}
\label{subsec:empirical_approach}

The practical solution involves a two-step iterative process:

\begin{definition}[Empirical Wiener Filter]
    \label{def:empirical_wiener}
    The empirical Wiener filter uses a preliminary estimate $\hat{\tilde{x}}_i^{(0)}$ to compute the signal variance, then applies the Wiener filter:
    \begin{align}
        \hat{\sigma}_{\tilde{x}_i}^2 & = \max(0, |\hat{\tilde{x}}_i^{(0)}|^2 - \sigma^2) \label{eq:empirical_variance}                           \\
        \hat{\alpha}_i               & = \frac{\hat{\sigma}_{\tilde{x}_i}^2}{\hat{\sigma}_{\tilde{x}_i}^2 + \sigma^2} \label{eq:empirical_alpha} \\
        \hat{\tilde{x}}_i^{(1)}      & = \hat{\alpha}_i \tilde{y}_i \label{eq:empirical_estimate}
    \end{align}
\end{definition}

\begin{remark}
    The max operation in Equation~\ref{eq:empirical_variance} ensures non-negative variance estimates, preventing numerical instabilities.
\end{remark}

\subsection{Iteration and Convergence}
\label{subsec:iteration}

The empirical Wiener filter can be applied iteratively:

\begin{align}
    \hat{\tilde{x}}_i^{(k+1)}          & = \hat{\alpha}_i^{(k)} \tilde{y}_i                                                                                   \\
    \text{where } \hat{\alpha}_i^{(k)} & = \frac{\max(0, |\hat{\tilde{x}}_i^{(k)}|^2 - \sigma^2)}{\max(0, |\hat{\tilde{x}}_i^{(k)}|^2 - \sigma^2) + \sigma^2}
\end{align}

\begin{proposition}[Convergence Property]
    \label{prop:convergence}
    For the linear case with Gaussian assumptions, the empirical Wiener filter typically converges within 2-3 iterations to a stable solution.
\end{proposition}

\newpage

% NOTE: This section connects theory to modern patch-based methods

\section{Patch-Based Wiener Filtering}
\label{sec:patch_based}

Modern image denoising often employs patch-based approaches that apply the Wiener filter to local image patches rather than the entire image.

\subsection{Patch Extraction and Processing}
\label{subsec:patch_processing}

\begin{definition}[Patch-Based Processing]
    \label{def:patch_processing}
    Given an image $\mathbf{Y}$ of size $M \times M$, we extract overlapping patches $\mathbf{P}_{i,j}$ of size $p \times p$ centered at pixel $(i,j)$:
    \begin{equation}
        \mathbf{P}_{i,j} = \mathbf{Y}[i-\lfloor p/2 \rfloor : i+\lfloor p/2 \rfloor, j-\lfloor p/2 \rfloor : j+\lfloor p/2 \rfloor]
    \end{equation}
\end{definition}

\subsection{Transform Domain Patch Processing}
\label{subsec:transform_patch}

For each patch $\mathbf{P}_{i,j}$, we apply the following procedure:

\begin{enumerate}[leftmargin=*]
    \item \textbf{Vectorization}: Convert patch to vector $\vec{p}_{i,j} \in \mathbb{R}^{p^2}$
    \item \textbf{Transformation}: Apply orthogonal transform $\tilde{\vec{p}}_{i,j} = \mathbf{T}\vec{p}_{i,j}$
    \item \textbf{Preliminary Estimation}: Obtain initial estimate $\hat{\tilde{\vec{p}}}_{i,j}^{(0)}$ using a simple denoising method
    \item \textbf{Wiener Filtering}: Apply empirical Wiener filter coefficient-wise
    \item \textbf{Inverse Transform}: Reconstruct patch $\hat{\vec{p}}_{i,j} = \mathbf{T}^{-1}\hat{\tilde{\vec{p}}}_{i,j}$
\end{enumerate}

\subsection{Aggregation Strategies}
\label{subsec:aggregation}

Since patches overlap, multiple estimates exist for each pixel. We aggregate these estimates using weighted averaging:

\begin{equation}
    \label{eq:aggregation}
    \hat{x}_{i,j} = \frac{\sum_{(k,l) \in \mathcal{N}_{i,j}} w_{k,l} \hat{p}_{k,l}^{(i,j)}}{\sum_{(k,l) \in \mathcal{N}_{i,j}} w_{k,l}}
\end{equation}

where:
\begin{itemize}[leftmargin=*]
    \item $\mathcal{N}_{i,j}$ is the set of patches containing pixel $(i,j)$
    \item $\hat{p}_{k,l}^{(i,j)}$ is the estimate of pixel $(i,j)$ from patch centered at $(k,l)$
    \item $w_{k,l}$ are aggregation weights
\end{itemize}

\subsection{Sparsity-Aware Weighting}
\label{subsec:sparsity_aware}

A sophisticated weighting scheme uses the sparsity of transform coefficients:

\begin{equation}
    \label{eq:sparsity_weight}
    w_{k,l} = \frac{1}{\norm{\hat{\tilde{\vec{p}}}_{k,l}}_0 + \epsilon}
\end{equation}

where $\norm{\cdot}_0$ denotes the number of non-zero elements (sparsity measure) and $\epsilon > 0$ prevents division by zero.

\begin{remark}
    Sparsity-aware weighting gives higher importance to patches with fewer significant coefficients, as these typically correspond to smoother regions with better signal-to-noise ratio.
\end{remark}

\newpage

% NOTE: This section provides comprehensive algorithm descriptions

\section{Complete Algorithm Description}
\label{sec:algorithm}

\subsection{Baseline Patch-Based Wiener Filter}
\label{subsec:baseline_algorithm}

\begin{algorithm}
    \caption{Wiener Filtering for Patch-based Image Denoising}
    \begin{algorithmic}[1]
        \Require Noisy image $\mathbf{Y}$, patch size $p$, noise variance $\sigma^2$, step size $s$
        \Ensure Denoised image $\hat{\mathbf{X}}$
        \State Initialize aggregation matrices $\mathbf{A} = \mathbf{0}$, $\mathbf{W} = \mathbf{0}$
        \For{each patch location $(i,j)$ with step $s$}
        \State Extract patch $\mathbf{P}_{i,j}$ from $\mathbf{Y}$
        \State Vectorize: $\vec{p}_{i,j} = \text{vec}(\mathbf{P}_{i,j})$
        \State Apply transform: $\tilde{\vec{p}}_{i,j} = \mathbf{T}\vec{p}_{i,j}$
        \State Compute initial estimate $\hat{\tilde{\vec{p}}}_{i,j}^{(0)}$ (e.g., hard thresholding)
        \For{each coefficient $k = 1, \ldots, p^2$}
        \State Compute empirical variance: $\hat{\sigma}_k^2 = \max(0, |\hat{\tilde{p}}_{i,j,k}^{(0)}|^2 - \sigma^2)$
        \State Compute Wiener coefficient: $\hat{\alpha}_k = \frac{\hat{\sigma}_k^2}{\hat{\sigma}_k^2 + \sigma^2}$
        \State Apply filter: $\hat{\tilde{p}}_{i,j,k} = \hat{\alpha}_k \tilde{p}_{i,j,k}$
        \EndFor
        \State Inverse transform: $\hat{\vec{p}}_{i,j} = \mathbf{T}^{-1}\hat{\tilde{\vec{p}}}_{i,j}$
        \State Reshape to patch: $\hat{\mathbf{P}}_{i,j} = \text{reshape}(\hat{\vec{p}}_{i,j}, p, p)$
        \State Compute patch weight $w_{i,j}$ (uniform or sparsity-aware)
        \State Aggregate: $\mathbf{A} \leftarrow \mathbf{A} + w_{i,j} \cdot \hat{\mathbf{P}}_{i,j}$
        \State Update weights: $\mathbf{W} \leftarrow \mathbf{W} + w_{i,j}$
        \EndFor
        \State Normalize: $\hat{\mathbf{X}} = \mathbf{A} \oslash \mathbf{W}$ \Comment{element-wise division}
    \end{algorithmic}
\end{algorithm}

\subsection{Advanced Considerations}
\label{subsec:advanced_considerations}

\subsubsection{Sliding Window vs. Block Processing}
\label{subsubsec:sliding_window}

Two main approaches for patch extraction:

\begin{itemize}[leftmargin=*]
    \item \textbf{Block Processing} ($s = p$): Non-overlapping patches, faster processing
    \item \textbf{Sliding Window} ($s = 1$): Maximum overlap, better denoising quality
\end{itemize}

\begin{proposition}[Quality vs. Complexity Trade-off]
    \label{prop:quality_complexity}
    Sliding window processing typically improves PSNR by 0.3-0.5 dB compared to block processing, at the cost of $p^2$ times more computational complexity.
\end{proposition}

\subsubsection{Noise Variance Estimation}
\label{subsubsec:noise_estimation}

Accurate noise variance estimation is crucial for optimal performance. Two common approaches:

\begin{enumerate}[leftmargin=*]
    \item \textbf{Robust Estimator}: $\hat{\sigma} = \frac{\text{median}(|\mathbf{D}|)}{0.6745}$ where $\mathbf{D}$ contains high-frequency wavelet coefficients
    \item \textbf{Method of Moments}: $\hat{\sigma}^2 = \frac{1}{N-1}\sum_{i=1}^N (y_i - \bar{y})^2$ for homogeneous regions
\end{enumerate}

\newpage

% NOTE: This section discusses performance metrics and experimental considerations

\section{Performance Analysis and Experimental Results}
\label{sec:performance}

\subsection{Quality Metrics}
\label{subsec:quality_metrics}

\begin{definition}[Peak Signal-to-Noise Ratio (PSNR)]
    \label{def:psnr}
    For 8-bit images, PSNR is defined as:
    \begin{equation}
        \text{PSNR} = 10 \log_{10}\left(\frac{255^2}{\text{MSE}}\right) \text{ dB}
    \end{equation}
    where $\text{MSE} = \frac{1}{MN}\sum_{i=1}^M\sum_{j=1}^N (x_{i,j} - \hat{x}_{i,j})^2$.
\end{definition}

\subsection{Comparative Analysis}
\label{subsec:comparative_analysis}

Typical performance improvements observed:

\begin{center}
    \begin{tabular}{@{}lcc@{}}
        \toprule
        Method                     & PSNR Improvement & Visual Quality                \\
        \midrule
        Simple Convolution         & -2.0 dB          & Poor (over-smoothing)         \\
        Block-wise Processing      & +1.5 dB          & Good (some artifacts)         \\
        Uniform Weight Aggregation & +2.0 dB          & Good                          \\
        Sparsity-Aware Aggregation & +2.3 dB          & Excellent (edge preservation) \\
        Sliding Window             & +2.5 dB          & Excellent                     \\
        \bottomrule
    \end{tabular}
\end{center}

\subsection{Threshold Selection Analysis}
\label{subsec:threshold_analysis}

The choice of threshold in preliminary estimation significantly affects performance:

\begin{itemize}[leftmargin=*]
    \item \textbf{Too small}: Insufficient noise removal, poor Wiener coefficient estimation
    \item \textbf{Too large}: Over-smoothing, loss of image details
    \item \textbf{Optimal}: Typically $\tau = 3\sigma$ for hard thresholding
\end{itemize}

\newpage

% NOTE: This section provides practical implementation guidance

\section{Implementation Guidelines and Best Practices}
\label{sec:implementation}

\subsection{Computational Complexity}
\label{subsec:computational_complexity}

\begin{definition}[Complexity Analysis]
    \label{def:complexity}
    For an $M \times M$ image with patch size $p \times p$ and step size $s$:
    \begin{itemize}[leftmargin=*]
        \item Number of patches: $\left\lceil\frac{M-p+1}{s}\right\rceil^2$
        \item Transform operations: $O(p^2 \log p)$ per patch (for FFT-based transforms)
        \item Total complexity: $O\left(\frac{M^2 p^2 \log p}{s^2}\right)$
    \end{itemize}
\end{definition}

\subsection{Memory Requirements}
\label{subsec:memory_requirements}

Efficient implementation requires careful memory management:

\begin{itemize}[leftmargin=*]
    \item \textbf{Patch storage}: $O(p^2)$ per patch
    \item \textbf{Aggregation buffers}: $O(M^2)$ for accumulation and weight matrices
    \item \textbf{Transform matrices}: $O(p^2)$ for precomputed transforms
\end{itemize}

\subsection{Numerical Stability Considerations}
\label{subsec:numerical_stability}

\begin{itemize}[leftmargin=*]
    \item Avoid division by zero in weight computation: use $\epsilon = 10^{-10}$
    \item Ensure positive variance estimates: use $\max(0, \cdot)$ operations
    \item Normalize aggregation weights to prevent overflow
\end{itemize}

\newpage

% NOTE: This appendix provides detailed mathematical derivations

\appendix

\section{Mathematical Derivations}
\label{app:derivations}

\subsection{Orthogonal Transform Properties}
\label{app:orthogonal_properties}

For an orthogonal matrix $\mathbf{T}$ (i.e., $\mathbf{T}\trans{\mathbf{T}} = \mathbf{I}$):

\begin{lemma}[Noise Preservation]
    \label{lemma:noise_preservation}
    If $\vec{n} \sim \mathcal{N}(\vec{0}, \sigma^2\mathbf{I})$, then $\tilde{\vec{n}} = \mathbf{T}\vec{n} \sim \mathcal{N}(\vec{0}, \sigma^2\mathbf{I})$.
\end{lemma}

\begin{proof}
    The covariance matrix of $\tilde{\vec{n}}$ is:
    \begin{align}
        \text{Cov}(\tilde{\vec{n}}) & = \E[\tilde{\vec{n}}\trans{\tilde{\vec{n}}}]             \\
                                    & = \E[\mathbf{T}\vec{n}\trans{\vec{n}}\trans{\mathbf{T}}] \\
                                    & = \mathbf{T}\E[\vec{n}\trans{\vec{n}}]\trans{\mathbf{T}} \\
                                    & = \mathbf{T}(\sigma^2\mathbf{I})\trans{\mathbf{T}}       \\
                                    & = \sigma^2\mathbf{T}\trans{\mathbf{T}}                   \\
                                    & = \sigma^2\mathbf{I}
    \end{align}
    Since $\tilde{\vec{n}}$ is a linear combination of Gaussian random variables, it remains Gaussian.
\end{proof}

\subsection{Variance Estimation Bias}
\label{app:variance_bias}

\begin{lemma}[Bias in Empirical Variance]
    \label{lemma:variance_bias}
    The empirical variance estimator $\hat{\sigma}_{\tilde{x}_i}^2 = \max(0, |\hat{\tilde{x}}_i^{(0)}|^2 - \sigma^2)$ is biased but consistent.
\end{lemma}

\begin{proof}
    For the unbiased case (ignoring the max operation):
    \begin{align}
        \E[|\hat{\tilde{x}}_i^{(0)}|^2 - \sigma^2] & = \E[|\hat{\tilde{x}}_i^{(0)}|^2] - \sigma^2
    \end{align}
    If $\hat{\tilde{x}}_i^{(0)}$ is an unbiased estimator of $\tilde{x}_i$, then:
    \begin{align}
        \E[|\hat{\tilde{x}}_i^{(0)}|^2] & = \text{Var}(\hat{\tilde{x}}_i^{(0)}) + (\E[\hat{\tilde{x}}_i^{(0)}])^2 \\
                                        & \approx \sigma_{\tilde{x}_i}^2 + \text{estimation error}
    \end{align}
    The max operation introduces bias, but as the estimation improves, the bias decreases.
\end{proof}

\newpage

% NOTE: This section provides symbol definitions and references


\end{document}